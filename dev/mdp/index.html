<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Markov Decision Processes ¬∑ VLDataScienceMachineLearningPackage</title><meta name="title" content="Markov Decision Processes ¬∑ VLDataScienceMachineLearningPackage"/><meta property="og:title" content="Markov Decision Processes ¬∑ VLDataScienceMachineLearningPackage"/><meta property="twitter:title" content="Markov Decision Processes ¬∑ VLDataScienceMachineLearningPackage"/><meta name="description" content="Documentation for VLDataScienceMachineLearningPackage."/><meta property="og:description" content="Documentation for VLDataScienceMachineLearningPackage."/><meta property="twitter:description" content="Documentation for VLDataScienceMachineLearningPackage."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">VLDataScienceMachineLearningPackage</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../data/">Data</a></li><li><a class="tocitem" href="../types/">Types</a></li><li><a class="tocitem" href="../factory/">Factory</a></li><li><a class="tocitem" href="../text/">Text</a></li><li><a class="tocitem" href="../wolfram/">Wolfram</a></li><li><a class="tocitem" href="../graphs/">Graphs</a></li><li><a class="tocitem" href="../solvers/">Linear Systems</a></li><li><a class="tocitem" href="../binaryclassification/">Binary Classification</a></li><li><span class="tocitem">Reinforcement Learning</span><ul><li class="is-active"><a class="tocitem" href>Markov Decision Processes</a><ul class="internal"><li><a class="tocitem" href="#Types"><span>Types</span></a></li><li><a class="tocitem" href="#Value-Iteration-Algorithm"><span>Value Iteration Algorithm</span></a></li><li><a class="tocitem" href="#Cobb-Douglas-Choice-Problem"><span>Cobb-Douglas Choice Problem</span></a></li></ul></li><li><a class="tocitem" href="../bandit/">Bandit Algorithms</a></li><li><a class="tocitem" href="../online/">Multiplicative Weight Updates</a></li><li><a class="tocitem" href="../qlearning/">Q-Learning</a></li></ul></li><li><span class="tocitem">Deep Learning</span><ul><li><a class="tocitem" href="../hopfield/">Hopfield Networks</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reinforcement Learning</a></li><li class="is-active"><a href>Markov Decision Processes</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Markov Decision Processes</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl/blob/main/docs/src/mdp.md" title="Edit source on GitHub"><span class="docs-icon fa-solid">ÔÅÑ</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Markov-Decision-Processes-(MDPs)"><a class="docs-heading-anchor" href="#Markov-Decision-Processes-(MDPs)">Markov Decision Processes (MDPs)</a><a id="Markov-Decision-Processes-(MDPs)-1"></a><a class="docs-heading-anchor-permalink" href="#Markov-Decision-Processes-(MDPs)" title="Permalink"></a></h1><p>We&#39;ve developed some codes to work with Markov Decision Processes (MDPs). MDPs are mathematical frameworks used for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. They are widely used in various fields, including robotics, economics, and artificial intelligence, particularly in reinforcement learning.</p><h2 id="Types"><a class="docs-heading-anchor" href="#Types">Types</a><a id="Types-1"></a><a class="docs-heading-anchor-permalink" href="#Types" title="Permalink"></a></h2><p>We have defined the following abstract and concrete types to represent MDPs and related concepts:</p><article><details class="docstring" open="true"><summary id="VLDataScienceMachineLearningPackage.MyMDPProblemModel"><a class="docstring-binding" href="#VLDataScienceMachineLearningPackage.MyMDPProblemModel"><code>VLDataScienceMachineLearningPackage.MyMDPProblemModel</code></a> ‚Äî <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">mutable struct MyMDPProblemModel &lt;: AbstractProcessModel</code></pre><p>A mutable struct that defines a Markov Decision Process (MDP) model.  The MDP model is defined by the tuple <code>(ùíÆ, ùíú, T, R, Œ≥)</code>.  The state space <code>ùíÆ</code> is an array of integers, the action space <code>ùíú</code> is an array of integers,  the transition matrix <code>T</code> is a function or a 3D array, the reward matrix <code>R</code> is a function or a 2D array,  and the discount factor <code>Œ≥</code> is a float.</p><p><strong>Fields</strong></p><ul><li><code>ùíÆ::Array{Int64,1}</code>: state space</li><li><code>ùíú::Array{Int64,1}</code>: action space</li><li><code>T::Union{Function, Array{Float64,3}}</code>: transition matrix of function</li><li><code>R::Union{Function, Array{Float64,2}}</code>: reward matrix or function</li><li><code>Œ≥::Float64</code>: discount factor</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl/blob/13d2f26d9a8b8ad058117248da2b4bd3dac8e1cb/src/Types.jl#L413-L428">source</a></section></details></article><article><details class="docstring" open="true"><summary id="VLDataScienceMachineLearningPackage.MyRectangularGridWorldModel"><a class="docstring-binding" href="#VLDataScienceMachineLearningPackage.MyRectangularGridWorldModel"><code>VLDataScienceMachineLearningPackage.MyRectangularGridWorldModel</code></a> ‚Äî <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">mutable struct MyRectangularGridWorldModel &lt;: AbstractWorldModel</code></pre><p>A mutable struct that defines a rectangular grid world model.</p><p><strong>Fields</strong></p><ul><li><code>number_of_rows::Int</code>: number of rows in the grid</li><li><code>number_of_cols::Int</code>: number of columns in the grid</li><li><code>coordinates::Dict{Int,Tuple{Int,Int}}</code>: dictionary of state to coordinate mapping</li><li><code>states::Dict{Tuple{Int,Int},Int}</code>: dictionary of coordinate to state mapping</li><li><code>moves::Dict{Int,Tuple{Int,Int}}</code>: dictionary of state to move mapping</li><li><code>rewards::Dict{Int,Float64}</code>: dictionary of state to reward mapping</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl/blob/13d2f26d9a8b8ad058117248da2b4bd3dac8e1cb/src/Types.jl#L442-L454">source</a></section></details></article><article><details class="docstring" open="true"><summary id="VLDataScienceMachineLearningPackage.MyValueIterationModel"><a class="docstring-binding" href="#VLDataScienceMachineLearningPackage.MyValueIterationModel"><code>VLDataScienceMachineLearningPackage.MyValueIterationModel</code></a> ‚Äî <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">struct MyValueIterationModel &lt;: AbstractProcessModel</code></pre><p>A struct that defines a value iteration model.  The value iteration model is defined by the maximum number of iterations <code>k_max</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl/blob/13d2f26d9a8b8ad058117248da2b4bd3dac8e1cb/src/Types.jl#L469-L474">source</a></section></details></article><article><details class="docstring" open="true"><summary id="VLDataScienceMachineLearningPackage.MyValueFunctionPolicy"><a class="docstring-binding" href="#VLDataScienceMachineLearningPackage.MyValueFunctionPolicy"><code>VLDataScienceMachineLearningPackage.MyValueFunctionPolicy</code></a> ‚Äî <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">struct MyValueFunctionPolicy</code></pre><p>A struct that defines a value function policy.</p><p><strong>Fields</strong></p><ul><li><code>problem::MyMDPProblemModel</code>: MDP problem model</li><li><code>U::Array{Float64,1}</code>: value function vector. This holds the Utility of each state.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl/blob/13d2f26d9a8b8ad058117248da2b4bd3dac8e1cb/src/Types.jl#L493-L501">source</a></section></details></article><article><details class="docstring" open="true"><summary id="VLDataScienceMachineLearningPackage.MyRandomRolloutModel"><a class="docstring-binding" href="#VLDataScienceMachineLearningPackage.MyRandomRolloutModel"><code>VLDataScienceMachineLearningPackage.MyRandomRolloutModel</code></a> ‚Äî <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">struct MyRandomRolloutModel</code></pre><p>A struct that defines a random rollout model. The random rollout model is defined by the depth of the rollout.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl/blob/13d2f26d9a8b8ad058117248da2b4bd3dac8e1cb/src/Types.jl#L481-L486">source</a></section></details></article><p>We construct the <code>MyMDProblemModel</code> type, and the <code>MyRectangularGridWorldModel</code> type to represent the environment in which the MDP operates using custom build methods. The <code>MyValueIterationModel</code>, <code>MyRandomRolloutModel</code>, and <code>MyValueFunctionPolicy</code> can be constructed using their default constructors.</p><h2 id="Value-Iteration-Algorithm"><a class="docs-heading-anchor" href="#Value-Iteration-Algorithm">Value Iteration Algorithm</a><a id="Value-Iteration-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Value-Iteration-Algorithm" title="Permalink"></a></h2><p>We have implemented the Value Iteration algorithm, which is a dynamic programming algorithm used to compute the optimal policy and value function for an MDP. The algorithm iteratively updates the value of each state based on the expected rewards and the values of successor states.</p><article><details class="docstring" open="true"><summary id="VLDataScienceMachineLearningPackage.solve-Tuple{MyValueIterationModel, MyMDPProblemModel}"><a class="docstring-binding" href="#VLDataScienceMachineLearningPackage.solve-Tuple{MyValueIterationModel, MyMDPProblemModel}"><code>VLDataScienceMachineLearningPackage.solve</code></a> ‚Äî <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">solve(model::MyValueIterationModel, problem::MyMDPProblemModel) -&gt; MyValueFunctionPolicy</code></pre><p>This function solves the MDP problem using value iteration.</p><p><strong>Arguments</strong></p><ul><li><code>model::MyValueIterationModel</code>: the value iteration model</li><li><code>problem::MyMDPProblemModel</code>: the MDP problem model</li></ul><p><strong>Returns</strong></p><ul><li><code>MyValueFunctionPolicy</code>: the value function policy</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl/blob/13d2f26d9a8b8ad058117248da2b4bd3dac8e1cb/src/MDP.jl#L201-L212">source</a></section></details></article><article><details class="docstring" open="true"><summary id="VLDataScienceMachineLearningPackage.backup-Tuple{MyMDPProblemModel, Vector{Float64}, Int64}"><a class="docstring-binding" href="#VLDataScienceMachineLearningPackage.backup-Tuple{MyMDPProblemModel, Vector{Float64}, Int64}"><code>VLDataScienceMachineLearningPackage.backup</code></a> ‚Äî <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">backup(problem::MyMDPProblemModel, U::Array{Float64,1}, s::Int64) -&gt; Float64</code></pre><p>This function computes the backup value for a given state <code>s</code> and value function <code>U</code>.</p><p><strong>Arguments</strong></p><ul><li><code>problem::MyMDPProblemModel</code>: the MDP problem model</li><li><code>U::Array{Float64,1}</code>: the value function vector</li><li><code>s::Int64</code>: the state</li></ul><p><strong>Returns</strong></p><ul><li><code>Float64</code>: the best backup value for the state <code>s</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl/blob/13d2f26d9a8b8ad058117248da2b4bd3dac8e1cb/src/MDP.jl#L184-L196">source</a></section></details></article><article><details class="docstring" open="true"><summary id="VLDataScienceMachineLearningPackage.policy-Tuple{Matrix{Float64}}"><a class="docstring-binding" href="#VLDataScienceMachineLearningPackage.policy-Tuple{Matrix{Float64}}"><code>VLDataScienceMachineLearningPackage.policy</code></a> ‚Äî <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">policy(Q_array::Array{Float64,2}) -&gt; Array{Int,1}</code></pre><p>This function computes the policy from the Q-value function.</p><p><strong>Arguments</strong></p><ul><li><code>Q_array::Array{Float64,2}</code>: the Q-value function</li></ul><p><strong>Returns</strong></p><ul><li><code>Array{Int,1}</code>: the policy which maps states to actions</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl/blob/13d2f26d9a8b8ad058117248da2b4bd3dac8e1cb/src/MDP.jl#L158-L168">source</a></section></details></article><article><details class="docstring" open="true"><summary id="VLDataScienceMachineLearningPackage.Q-Tuple{MyMDPProblemModel, Vector{Float64}}"><a class="docstring-binding" href="#VLDataScienceMachineLearningPackage.Q-Tuple{MyMDPProblemModel, Vector{Float64}}"><code>VLDataScienceMachineLearningPackage.Q</code></a> ‚Äî <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">Q(p::MyMDPProblemModel, U::Array{Float64,1}) -&gt; Array{Float64,2}</code></pre><p>This function computes the Q-value function for a given value function <code>U</code>.</p><p><strong>Arguments</strong></p><ul><li><code>p::MyMDPProblemModel</code>: the MDP problem model</li><li><code>U::Array{Float64,1}</code>: the value function vector</li></ul><p><strong>Returns</strong></p><ul><li><code>Array{Float64,2}</code>: the Q-value function</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl/blob/13d2f26d9a8b8ad058117248da2b4bd3dac8e1cb/src/MDP.jl#L117-L128">source</a></section></details></article><article><details class="docstring" open="true"><summary id="VLDataScienceMachineLearningPackage.lookahead"><a class="docstring-binding" href="#VLDataScienceMachineLearningPackage.lookahead"><code>VLDataScienceMachineLearningPackage.lookahead</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">lookahead(p::MyMDPProblemModel, U::Vector{Float64}, s::Int64, a::Int64)</code></pre><p>This function computes the lookahead value for a given state-action pair <code>(s,a)</code>.  It uses a vector <code>U</code> to compute the value function.</p><p><strong>Arguments</strong></p><ul><li><code>p::MyMDPProblemModel</code>: the MDP problem model</li><li><code>U::Vector{Float64}</code>: the value function vector</li><li><code>s::Int64</code>: the state</li><li><code>a::Int64</code>: the action</li></ul><p><strong>Returns</strong></p><ul><li><code>Float64</code>: the lookahead value for the state-action pair <code>(s,a)</code>. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl/blob/13d2f26d9a8b8ad058117248da2b4bd3dac8e1cb/src/MDP.jl#L76-L90">source</a></section><section><div><pre><code class="language-julia hljs">lookahead(p::MyMDPProblemModel, U::Function, s::Int64, a::Int64)::Float64</code></pre><p>This function computes the lookahead value for a given state-action pair <code>(s,a)</code>.  It uses a function <code>U</code> to compute the value function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl/blob/13d2f26d9a8b8ad058117248da2b4bd3dac8e1cb/src/MDP.jl#L103-L108">source</a></section></details></article><article><details class="docstring" open="true"><summary id="VLDataScienceMachineLearningPackage.iterative_policy_evaluation"><a class="docstring-binding" href="#VLDataScienceMachineLearningPackage.iterative_policy_evaluation"><code>VLDataScienceMachineLearningPackage.iterative_policy_evaluation</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">iterative_policy_evaluation(p::MyMDPProblemModel, œÄ, k_max::Int) -&gt; Array{Float64,1}</code></pre><p>This function performs iterative policy evaluation for a given MDP problem and policy.</p><p><strong>Arguments</strong></p><ul><li><code>p::MyMDPProblemModel</code>: the MDP problem model</li><li><code>œÄ</code>: the policy function</li><li><code>k_max::Int</code>: the maximum number of iterations</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl/blob/13d2f26d9a8b8ad058117248da2b4bd3dac8e1cb/src/MDP.jl#L282-L292">source</a></section></details></article><article><details class="docstring" open="true"><summary id="VLDataScienceMachineLearningPackage.greedy"><a class="docstring-binding" href="#VLDataScienceMachineLearningPackage.greedy"><code>VLDataScienceMachineLearningPackage.greedy</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">greedy(problem::MyMDPProblemModel, U::Array{Float64,1}, s::Int64) -&gt; (a::Int64, u::Float64)</code></pre><p>This function computes the greedy action and its value for a given state <code>s</code> and value function <code>U</code>.</p><p><strong>Arguments</strong></p><ul><li><code>problem::MyMDPProblemModel</code>: the MDP problem model</li><li><code>U::Array{Float64,1}</code>: the value function vector</li><li><code>s::Int64</code>: the state</li></ul><p><strong>Returns</strong></p><ul><li><code>(a::Int64, u::Float64)</code>: a tuple of the best action and its value</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl/blob/13d2f26d9a8b8ad058117248da2b4bd3dac8e1cb/src/MDP.jl#L311-L323">source</a></section></details></article><article><details class="docstring" open="true"><summary id="VLDataScienceMachineLearningPackage.myrandpolicy-Tuple{MyMDPProblemModel, MyRectangularGridWorldModel, Int64}"><a class="docstring-binding" href="#VLDataScienceMachineLearningPackage.myrandpolicy-Tuple{MyMDPProblemModel, MyRectangularGridWorldModel, Int64}"><code>VLDataScienceMachineLearningPackage.myrandpolicy</code></a> ‚Äî <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">myrandpolicy(problem::MyMDPProblemModel, world::MyRectangularGridWorldModel, s::Int)::Int</code></pre><p>This function implements a random policy for a given MDP problem and world model.</p><p><strong>Arguments</strong></p><ul><li><code>problem::MyMDPProblemModel</code>: the MDP problem model</li><li><code>world::MyRectangularGridWorldModel</code>: the world model</li><li><code>s::Int</code>: the state</li></ul><p><strong>Returns</strong></p><ul><li><code>Int</code>: the action we choose</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl/blob/13d2f26d9a8b8ad058117248da2b4bd3dac8e1cb/src/MDP.jl#L1-L13">source</a></section></details></article><article><details class="docstring" open="true"><summary id="VLDataScienceMachineLearningPackage.myrandstep-Tuple{MyMDPProblemModel, MyRectangularGridWorldModel, Int64, Int64}"><a class="docstring-binding" href="#VLDataScienceMachineLearningPackage.myrandstep-Tuple{MyMDPProblemModel, MyRectangularGridWorldModel, Int64, Int64}"><code>VLDataScienceMachineLearningPackage.myrandstep</code></a> ‚Äî <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">myrandstep(problem::MyMDPProblemModel, world::MyRectangularGridWorldModel, s::Int, a::Int)</code></pre><p>This function implements a random step for a given MDP problem and world model.</p><p><strong>Arguments</strong></p><ul><li><code>problem::MyMDPProblemModel</code>: the MDP problem model</li><li><code>world::MyRectangularGridWorldModel</code>: the world model</li><li><code>s::Int</code>: the state we are in</li><li><code>a::Int</code>: the action we choose</li></ul><p><strong>Returns</strong></p><ul><li><code>(s‚Ä≤,r)</code>: a tuple of the next state and the reward for being in state <code>s</code> and taking action <code>a</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl/blob/13d2f26d9a8b8ad058117248da2b4bd3dac8e1cb/src/MDP.jl#L41-L54">source</a></section></details></article><article><details class="docstring" open="true"><summary id="VLDataScienceMachineLearningPackage.solve-Tuple{MyRandomRolloutModel, MyMDPProblemModel, MyRectangularGridWorldModel, Int64}"><a class="docstring-binding" href="#VLDataScienceMachineLearningPackage.solve-Tuple{MyRandomRolloutModel, MyMDPProblemModel, MyRectangularGridWorldModel, Int64}"><code>VLDataScienceMachineLearningPackage.solve</code></a> ‚Äî <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">solve(model::MyRandomRolloutModel, problem::MyMDPProblemModel, 
    world::MyRectangularGridWorldModel, s::Int64) -&gt; Float64</code></pre><p>This function solves the MDP problem using random rollouts.</p><p><strong>Arguments</strong></p><ul><li><code>model::MyRandomRolloutModel</code>: the random rollout model</li><li><code>problem::MyMDPProblemModel</code>: the MDP problem model</li><li><code>world::MyRectangularGridWorldModel</code>: the world model</li><li><code>s::Int64</code>: the state</li></ul><p><strong>Returns</strong></p><ul><li><code>Float64</code>: the estimated utility value of the state <code>s</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl/blob/13d2f26d9a8b8ad058117248da2b4bd3dac8e1cb/src/MDP.jl#L229-L243">source</a></section></details></article><h2 id="Cobb-Douglas-Choice-Problem"><a class="docs-heading-anchor" href="#Cobb-Douglas-Choice-Problem">Cobb-Douglas Choice Problem</a><a id="Cobb-Douglas-Choice-Problem-1"></a><a class="docs-heading-anchor-permalink" href="#Cobb-Douglas-Choice-Problem" title="Permalink"></a></h2><p>We have also implemented a simple Cobb-Douglas choice problem, which is a type of economic model used to represent consumer preferences and choices. The Cobb-Douglas utility function is commonly used in economics to model the relationship between consumption of goods and overall utility.</p><article><details class="docstring" open="true"><summary id="VLDataScienceMachineLearningPackage.MySimpleCobbDouglasChoiceProblem"><a class="docstring-binding" href="#VLDataScienceMachineLearningPackage.MySimpleCobbDouglasChoiceProblem"><code>VLDataScienceMachineLearningPackage.MySimpleCobbDouglasChoiceProblem</code></a> ‚Äî <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">mutable struct MySimpleCobbDouglasChoiceProblem</code></pre><p>A model for a Cobb-Douglas choice problem. </p><p><strong>Fields</strong></p><ul><li><code>Œ±::Array{Float64,1}</code>: the vector of parameters for the Cobb-Douglas utility function (preferences)</li><li><code>c::Array{Float64,1}</code>: the vector of unit prices for the goods</li><li><code>I::Float64</code>: the income the consumer has to spend</li><li><code>bounds::Array{Float64,2}</code>: the bounds on the goods [0,U] where U is the upper bound</li><li><code>initial::Array{Float64,1}</code>: the initial guess for the solution</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl/blob/13d2f26d9a8b8ad058117248da2b4bd3dac8e1cb/src/Types.jl#L508-L519">source</a></section></details></article><article><details class="docstring" open="true"><summary id="VLDataScienceMachineLearningPackage.solve-Tuple{MySimpleCobbDouglasChoiceProblem}"><a class="docstring-binding" href="#VLDataScienceMachineLearningPackage.solve-Tuple{MySimpleCobbDouglasChoiceProblem}"><code>VLDataScienceMachineLearningPackage.solve</code></a> ‚Äî <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">solve(problem::MySimpleCobbDouglasChoiceProblem)</code></pre><p>Solve the Cobb-Douglas choice problem and return the results as a dictionary.</p><p><strong>Arguments</strong></p><ul><li><code>problem::MySimpleCobbDouglasChoiceProblem</code>: the Cobb-Douglas choice problem</li></ul><p><strong>Returns</strong></p><ul><li><code>Dict{String,Any}</code>: a dictionary with the results. The dictionary has the following keys:<ul><li><code>argmax::Array{Float64,1}</code>: the optimal choice of goods</li><li><code>budget::Float64</code>: the budget used</li><li><code>objective_value::Float64</code>: the value of the objective function</li></ul></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl/blob/13d2f26d9a8b8ad058117248da2b4bd3dac8e1cb/src/Solvers.jl#L272-L285">source</a></section></details></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../binaryclassification/">¬´ Binary Classification</a><a class="docs-footer-nextpage" href="../bandit/">Bandit Algorithms ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Sunday 7 December 2025 16:19">Sunday 7 December 2025</span>. Using Julia version 1.11.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
