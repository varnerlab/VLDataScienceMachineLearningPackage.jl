var documenterSearchIndex = {"docs":
[{"location":"boltzmann/#Boltzmann-Machines","page":"Boltzmann Machines","title":"Boltzmann Machines","text":"A Boltzmann Machine is a type of stochastic recurrent neural network that can learn a probability distribution over its set of inputs. It consists of a network of symmetrically connected, neuron-like units that make stochastic decisions about whether to be on or off. Boltzmann Machines are particularly useful for unsupervised learning tasks, such as dimensionality reduction, feature learning, and generative modeling.\n\nWe have types and methods for working with classical Boltzmann Machines in the VLDataScienceMachineLearningPackage.jl package.","category":"section"},{"location":"boltzmann/#Restricted-Boltzmann-Machines-(RBMs)","page":"Boltzmann Machines","title":"Restricted Boltzmann Machines (RBMs)","text":"A Restricted Boltzmann Machine (RBM) is a special type of Boltzmann Machine that has a bipartite structure, meaning that its neurons are divided into two layers: a visible layer and a hidden layer. There are no connections between neurons within the same layer, which simplifies the learning algorithm and makes RBMs more efficient to train.","category":"section"},{"location":"boltzmann/#VLDataScienceMachineLearningPackage.MySimpleBoltzmannMachineModel","page":"Boltzmann Machines","title":"VLDataScienceMachineLearningPackage.MySimpleBoltzmannMachineModel","text":"MySimpleBoltzmannMachineModel <: AbstractBoltzmannMachineModel\n\nA minimal Boltzmann machine model storing weights and biases.\n\nFields\n\nW::Array{Float64, 2}: symmetric weight matrix between units.\nb::Vector{Float64}: bias vector for each unit.\n\n\n\n\n\n","category":"type"},{"location":"boltzmann/#VLDataScienceMachineLearningPackage.sample-Tuple{MySimpleBoltzmannMachineModel, Vector{Int64}}","page":"Boltzmann Machines","title":"VLDataScienceMachineLearningPackage.sample","text":"sample(model::MySimpleBoltzmannMachineModel, sâ‚’::Vector{Int}; T::Int = 100, Î²::Float64 = 1.0) -> Array{Int,2}\n\nSimulate asynchronous Gibbs updates for a simple Boltzmann machine starting from an initial spin configuration.\n\nArguments\n\nmodel::MySimpleBoltzmannMachineModel: model containing weight matrix W and bias vector b.\nsâ‚’::Vector{Int}: initial spin state vector (+1/-1) for each neuron.\nT::Int = 100: number of time steps to simulate; each column in the output records a step.\nÎ²::Float64 = 1.0: inverse temperature controlling flip probabilities.\n\nReturns\n\nS::Array{Int,2}: matrix of spin states over time (neurons Ã— time).\n\n\n\n\n\n","category":"method"},{"location":"boltzmann/#VLDataScienceMachineLearningPackage.MyRestrictedBoltzmannMachineModel","page":"Boltzmann Machines","title":"VLDataScienceMachineLearningPackage.MyRestrictedBoltzmannMachineModel","text":"MyRestrictedBoltzmannMachineModel <: AbstractBoltzmannMachineModel\n\nA restricted Boltzmann machine (RBM) model storing weights and biases for visible and hidden units.\n\nFields\n\nW::Array{Float64, 2}: weight matrix between visible and hidden units.\nb::Vector{Float64}: bias vector for hidden units.\na::Vector{Float64}: bias vector for visible units.\n\n\n\n\n\n","category":"type"},{"location":"boltzmann/#VLDataScienceMachineLearningPackage.sample-Tuple{MyRestrictedBoltzmannMachineModel, Vector{Int64}}","page":"Boltzmann Machines","title":"VLDataScienceMachineLearningPackage.sample","text":"sample(model::MyRestrictedBoltzmannMachineModel, vâ‚’::Vector{Int}; \n    T::Int = 100, Î²::Float64 = 1.0)\n\nSample from a Restricted Boltzmann Machine (RBM) model. This does a forward pass and a feedback pass to sample the visible and hidden states.\n\nArguments\n\nmodel::MyRestrictedBoltzmannMachineModel: the RBM model to be simulated.\nvâ‚’::Vector{Int}: the initial visible state.\nT::Int: number of internal steps for sampling.\nÎ²::Float64: inverse temperature parameter.\n\nReturns\n\n(v, h): a tuple containing the sampled visible state v and the hidden state h.\n\n\n\n\n\n","category":"method"},{"location":"boltzmann/#VLDataScienceMachineLearningPackage.learn-Tuple{MyRestrictedBoltzmannMachineModel, Matrix{Int64}, Categorical{P} where P<:Real}","page":"Boltzmann Machines","title":"VLDataScienceMachineLearningPackage.learn","text":"learn(model::MyRestrictedBoltzmannMachineModel, data::Array{Int64,2}, p::Categorical;\n    maxnumberofiterations::Int = 100, T::Int = 100, Î²::Float64 = 1.0, batchsize::Int = 10, Î·::Float64 = 0.01,\n        tol::Float64 = 1e-6, verbose::Bool = true) -> MyRestrictedBoltzmannMachineModel\n\nTrain a Restricted Boltzmann Machine (RBM) model using Contrastive Divergence (CD) algorithm.\n\nArguments\n\nmodel::MyRestrictedBoltzmannMachineModel: the RBM model to be trained.\ndata::Array{Int64,2}: the training data, a matrix of size (numberofvisibleneurons, numberof_samples).\np::Categorical: a categorical distribution for sampling indices.\nmaxnumberofiterations::Int: maximum number of iterations for training.\nT::Int: number of internal steps for sampling.\nÎ²::Float64: inverse temperature parameter.\nbatchsize::Int: size of the batch for training.\nÎ·::Float64: learning rate.\ntol::Float64: relative parameter change tolerance for early stopping.\nverbose::Bool: whether to print progress information.\n\nReturn\n\nMyRestrictedBoltzmannMachineModel: the trained RBM model.\n\n\n\n\n\n","category":"method"},{"location":"binaryclassification/#Binary-Classification","page":"Binary Classification","title":"Binary Classification","text":"We've implemented some basic functionality for binary classification tasks, including a simple perceptron model and logistic regression. This model can be trained on labeled data and used to classify new instances.","category":"section"},{"location":"binaryclassification/#K-Nearest-Neighbors-Classifier","page":"Binary Classification","title":"K-Nearest Neighbors Classifier","text":"In addition to the perceptron and logistic regression models, we've also included a K-Nearest Neighbors (KNN) classifier. This model classifies instances based on the majority class of their nearest neighbors in the feature space.\n\nYou can call the classify function with a test data vector and a MyKNNClassificationModel instance to get the predicted class label.","category":"section"},{"location":"binaryclassification/#VLDataScienceMachineLearningPackage.learn","page":"Binary Classification","title":"VLDataScienceMachineLearningPackage.learn","text":"learn(features::Array{<:Number,2}, labels::Array{<:Number,1}, algorithm::AbstractClassificationAlgorithm; \n    maxiter::Int64 = 100, verbose::Bool = false)\n\nThe function learns a classification model from the data provided using the algorithm specified. This is a wrapper function that calls the internal function _learn whose implementation is algorithm-specific.\n\nArguments\n\nfeatures::Array{<:Number,2}: the features.\nlabels::Array{<:Number,1}: the labels.\nalgorithm::AbstractClassificationAlgorithm: the algorithm to use to learn the model.\n\nReturns\n\nthe updated algorithm model.\n\n\n\n\n\nlearn(model::MyRestrictedBoltzmannMachineModel, data::Array{Int64,2}, p::Categorical;\n    maxnumberofiterations::Int = 100, T::Int = 100, Î²::Float64 = 1.0, batchsize::Int = 10, Î·::Float64 = 0.01,\n        tol::Float64 = 1e-6, verbose::Bool = true) -> MyRestrictedBoltzmannMachineModel\n\nTrain a Restricted Boltzmann Machine (RBM) model using Contrastive Divergence (CD) algorithm.\n\nArguments\n\nmodel::MyRestrictedBoltzmannMachineModel: the RBM model to be trained.\ndata::Array{Int64,2}: the training data, a matrix of size (numberofvisibleneurons, numberof_samples).\np::Categorical: a categorical distribution for sampling indices.\nmaxnumberofiterations::Int: maximum number of iterations for training.\nT::Int: number of internal steps for sampling.\nÎ²::Float64: inverse temperature parameter.\nbatchsize::Int: size of the batch for training.\nÎ·::Float64: learning rate.\ntol::Float64: relative parameter change tolerance for early stopping.\nverbose::Bool: whether to print progress information.\n\nReturn\n\nMyRestrictedBoltzmannMachineModel: the trained RBM model.\n\n\n\n\n\n","category":"function"},{"location":"binaryclassification/#VLDataScienceMachineLearningPackage.classify","page":"Binary Classification","title":"VLDataScienceMachineLearningPackage.classify","text":"classify(features::Array{<:Number,2}, algorithm::AbstractClassificationAlgorithm)\n\n\n\n\n\nclassify(test::Array{<:Number,1}, algorithm::AbstractClassificationAlgorithm)\n\n\n\n\n\n","category":"function"},{"location":"binaryclassification/#VLDataScienceMachineLearningPackage.confusion","page":"Binary Classification","title":"VLDataScienceMachineLearningPackage.confusion","text":"confusion(actual::Array{Int64,1}, model::Array{Int64,1}) -> Array{Int64,2}\n\nThe function computes the confusion matrix for the classification model.\n\nArguments\n\nactual::Array{<:Number,1}: the actual labels.\nmodel::Array{<:Number,1}: the model estimated labels.\n\nReturns\n\na 2x2 confusion matrix. The rows correspond to the actual labels and the columns correspond to the predicted labels.\n\n\n\n\n\n","category":"function"},{"location":"binaryclassification/#VLDataScienceMachineLearningPackage.MyKNNClassificationModel","page":"Binary Classification","title":"VLDataScienceMachineLearningPackage.MyKNNClassificationModel","text":"mutable struct MyKNNClassificationModel <: AbstractClassificationAlgorithm\n\nA mutable struct that represents a K-Nearest Neighbors (KNN) classification model.\n\nFields\n\n- `K::Int64`: number of neighbours to look at\n- `d::Function`: similarity function\n- `X::Matrix{Float64}`: training data\n- `y::Vector{Int64}`: training labels\n\n\n\n\n\n","category":"type"},{"location":"data/#Data","page":"Data","title":"Data","text":"We've included several datasets in the package that we use for examples, activities, etc. ","category":"section"},{"location":"data/#Financial-Data","page":"Data","title":"Financial Data","text":"We have included a dataset of market data that we use for examples and activities in the financial domain. This  dataset holds the daily open, high, low, close, and volume data for a selection of stocks between 2014 and 2024.  In addition, we have defined a few methods for working with this data.","category":"section"},{"location":"data/#VLDataScienceMachineLearningPackage.MyKaggleCustomerSpendingDataset","page":"Data","title":"VLDataScienceMachineLearningPackage.MyKaggleCustomerSpendingDataset","text":"MyKaggleCustomerSpendingDataset() -> DataFrame\n\nLoad the Kaggle customer spending dataset as a DataFrame.  The original dataset can be found at: Spending dataset.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MyStringDecodeChallengeDataset","page":"Data","title":"VLDataScienceMachineLearningPackage.MyStringDecodeChallengeDataset","text":"MyStringDecodeChallengeDataset() -> NamedTuple\n\nLoad the String Decode Challenge testing and production datasets. \n\nReturn\n\nNamedTuple: A tuple containing the three datasets:\ntest_part_1: The first part of the test dataset.\ntest_part_2: The second part of the test dataset.\nproduction: The production dataset.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MyCommonSurnameDataset","page":"Data","title":"VLDataScienceMachineLearningPackage.MyCommonSurnameDataset","text":"MyCommonSurnameDataset() -> DataFrame\n\nLoad the common surnames dataset by country as a DataFrame. The original dataset can be found at: Common Surnames by Country.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MyCommonForenameDataset","page":"Data","title":"VLDataScienceMachineLearningPackage.MyCommonForenameDataset","text":"MyCommonForenameDataset() -> DataFrame\n\nLoad the common forenames dataset by country as a DataFrame. The original dataset can be found at: Common Forenames by Country.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MyKaggleHousingPricesDataset","page":"Data","title":"VLDataScienceMachineLearningPackage.MyKaggleHousingPricesDataset","text":"MyHousingPricesDataset() -> DataFrame\n\nLoad the house prices dataset from Kaggle as a DataFrame. The original dataset can be found at: Housing Prices Dataset on Kaggle\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MyGrayscaleSimpsonsImageDataset","page":"Data","title":"VLDataScienceMachineLearningPackage.MyGrayscaleSimpsonsImageDataset","text":"MyGrayscaleSimpsonsImageDataset() -> Dict{Int64, Array{Gray{N0f8},2}}\n\nLoad the Simpsons images dataset as a dictionary of grayscale images. This dataset contains 1000 images of Simpsons characters, each being 200 x 200 pixel images. These images are taken from the Simpsons Faces Dataset on Kaggle.\n\nReturns\n\nDict{Int64, Array{Gray{N0f8},2}}: A dictionary where the keys are image indices and the values are 2D arrays representing grayscale images.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MyUncorreleatedBlackAndWhiteImageDataset","page":"Data","title":"VLDataScienceMachineLearningPackage.MyUncorreleatedBlackAndWhiteImageDataset","text":"MyUncorreleatedBlackAndWhiteImageDataset() -> Array{Gray{N0f8},3}\n\nLoad the uncorrelated black and white images dataset as a 3D array. This dataset contains 100 images of size 28 x 28 pixels.\n\nReturns\n\nArray{Gray{N0f8},3}: A 3D array where each slice along the third dimension represents a grayscale image.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MyMNISTHandwrittenDigitImageDataset","page":"Data","title":"VLDataScienceMachineLearningPackage.MyMNISTHandwrittenDigitImageDataset","text":"MyMNISTHandwrittenDigitImageDataset(; number_of_training_examples::Int64 = 1000) -> Dict{Int64, Array{Gray{N0f8},3}}\n\nLoad the MNIST digits dataset as a dictionary of grayscale images. This dataset contains images of handwritten digits (0-9), each being 28 x 28 pixel images. The images were taken from the MNIST dataset.\n\nArguments\n\nnumber_of_examples::Int64 = 1000: The number of training examples to load for each digit (0-9). Default is 1000.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MyUSPSHandwrittenDigitImageDataset","page":"Data","title":"VLDataScienceMachineLearningPackage.MyUSPSHandwrittenDigitImageDataset","text":"MyUSPSHandwrittenDigitImageDataset() -> NamedTuple\n\nLoad the USPS handwritten digit image dataset as a NamedTuple containing records and labels.\n\nReturns\n\nNamedTuple: A tuple containing:\nrecords: A dictionary where keys are record indices and values are arrays of Float64 representing the digit images.\nlabels: An array of Int labels corresponding to each record.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MySarcasmCorpus","page":"Data","title":"VLDataScienceMachineLearningPackage.MySarcasmCorpus","text":"function MySarcasmCorpus() -> MySarcasmRecordCorpusModel\n\nThe function corpus reads a file composed of JSON records and returns the data as a MySarcasmRecordCorpusModel instance. Each record in the file is expected to have the following fields:\n\nis_sarcastic::Bool - a boolean value indicating if the headline is sarcastic.\nheadline::String - the headline of the article.\narticle_link::String - the link to the article.\n\nReturns\n\nMySarcasmRecordCorpusModel - the data from the file as a MySarcasmRecordCorpusModel instance.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MySMSSpamHamCorpus","page":"Data","title":"VLDataScienceMachineLearningPackage.MySMSSpamHamCorpus","text":"function MySMSSpamHamCorpus() -> MySMSSpamHamRecordCorpusModel\n\nThe function MySMSSpamHamCorpus reads the SMS Spam Ham dataset and returns the data as a MySMSSpamHamRecordCorpusModel instance.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MyGraphEdgeModels","page":"Data","title":"VLDataScienceMachineLearningPackage.MyGraphEdgeModels","text":"function MyGraphEdgeModels(filepath::String, edgeparser::Function; comment::Char='#', \ndelim::Char=',')::Dict{Int64,MyGraphEdgeModel}\n\nFunction to parse an edge file and return a dictionary of edges models.\n\nArguments\n\nfilepath::String: The path to the edge file.\nedgeparser::Function: A callback function to parse each edge line. This function should take a line as input, and a delimiter character, and return a tuple of the form (source, target, data), where:\nsource::Int64: The source node ID.\ntarget::Int64: The target node ID.\ndata::Any: Any additional data associated with the edge, e.g., a weight, a tuple of information, etc.\n\nReturns\n\nDict{Int64,MyGraphEdgeModel}: A dictionary of edge models.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MyConstrainedGraphEdgeModels","page":"Data","title":"VLDataScienceMachineLearningPackage.MyConstrainedGraphEdgeModels","text":"function MyConstrainedGraphEdgeModels(filepath::String, edgeparser::Function; comment::Char='#', \n    delim::Char=',') -> Dict{Int64,MyConstrainedGraphEdgeModel}\n\nThis function parses a constrained graph edge file and returns a dictionary of constrained graph edge models.\n\nArguments\n\nfilepath::String: The path to the edge file.\nedgeparser::Function: A callback function to parse each edge line. This function should take a line as input, and a delimiter character, and return a tuple of the form (source, target, weight, lower, upper), where:\nsource::Int64: The source node ID.\ntarget::Int64: The target node ID.\nweight::Union{Nothing, Number}: The weight of the edge.\nlower::Union{Nothing, Number}: The lower bound of the edge weight.\nupper::Union{Nothing, Number}: The upper bound of the edge weight.\n\nReturns\n\nDict{Int64,MyConstrainedGraphEdgeModel}: A dictionary of constrained graph edge models.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MyBanknoteAuthenticationDataset","page":"Data","title":"VLDataScienceMachineLearningPackage.MyBanknoteAuthenticationDataset","text":"MyBanknoteAuthenticationDataset() -> DataFrame\n\nThe second dataset we will explore is the banknote authentication dataset from the UCI archive.  This dataset has 1372 instances of 4 continuous features and an integer (-1,1) class variable. \n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MyEnglishLanguageVocabularyModel","page":"Data","title":"VLDataScienceMachineLearningPackage.MyEnglishLanguageVocabularyModel","text":"MyEnglishLanguageVocabularyModel() -> Dict{Char, Set{String}}\n\nLoad the English language vocabulary model as a dictionary where the keys are characters (the first letter of each word)  and the values are sets of words that start with that letter.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MyTrainingMarketDataSet","page":"Data","title":"VLDataScienceMachineLearningPackage.MyTrainingMarketDataSet","text":"MyTrainingMarketDataSet() -> Dict{String, DataFrame}\n\nLoad the components of the SP500 Daily open, high, low, close (OHLC) dataset as a dictionary of DataFrames. This data was provided by Polygon.io and covers the period from January 3, 2014, to December 31, 2024.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.log_growth_matrix","page":"Data","title":"VLDataScienceMachineLearningPackage.log_growth_matrix","text":"function log_growth_matrix(dataset::Dict{String, DataFrame}, \n            firms::Array{String,1}; Î”t::Float64 = (1.0/252.0), risk_free_rate::Float64 = 0.0) -> Array{Float64,2}\n\nThe log_growth_matrix function computes the excess log growth matrix for a given set of firms where we define the log growth as:\n\n    mu_tt-1(r_f) = frac1Delta t logleft(fracS_tS_t-1right) - r_f\n\nwhere S_t is the volume weighted average price (units: USD/share) at time t, Delta t is the time increment (in years), and r_f is the annual risk-free rate (units: 1/years) assuming continuous compounding.\n\nArguments\n\ndataset::Dict{String, DataFrame}: A dictionary of data frames where the keys are the firm ticker symbols and the values are the data frames holding price data. We use the volume_weighted_average_price column to compute the log growth by default.\nfirms::Array{String,1}: An array of firm ticker symbols for which we want to compute the log growth matrix.\nÎ”t::Float64: The time increment used to compute the log growth. The default value is 1/252, i.e., one trading day in units of years.\nrisk_free_rate::Float64: The risk-free rate used to compute the log growth. The default value is 0.0.\nkeycol::Symbol: The column in the data frame to use to compute the log growth. The default value is :volume_weighted_average_price.\ntestfirm::String: The firm ticker symbol to use to determine the number of trading days. By default, we use \"AAPL\".\n\nReturns\n\nArray{Float64,2}: An array of the excess log growth values for the given set of firms. The time series is the rows and the firms are the columns. The columns are ordered according to the order of the firms array.\n\nSee:\n\nThe DataFrame type (and methods for working with data frames) is exported from the DataFrames.jl package\n\n\n\n\n\n","category":"function"},{"location":"qlearning/#Q-Learning","page":"Q-Learning","title":"Q-Learning","text":"Q-Learning is a model-free reinforcement learning algorithm that aims to learn the value of taking a particular action in a given state. It does this by iteratively updating a Q-value table based on the agent's experiences in the environment.","category":"section"},{"location":"qlearning/#VLDataScienceMachineLearningPackage.MyQLearningAgentModel","page":"Q-Learning","title":"VLDataScienceMachineLearningPackage.MyQLearningAgentModel","text":"mutable struct MyQLearningAgentModel <: AbstractOnlineLearningModel\n\nA mutable type for the Q-Learning Agent model.\n\nFields\n\nstates::Array{Int,1}: array of states\nactions::Array{Int,1}: array of actions\nÎ³::Float64: discount factor\nÎ±::Float64: learning rate\nQ::Array{Float64,2}: Q-value table\n\n\n\n\n\n","category":"type"},{"location":"qlearning/#VLDataScienceMachineLearningPackage.solve-Tuple{MyQLearningAgentModel, MyRectangularGridWorldModel}","page":"Q-Learning","title":"VLDataScienceMachineLearningPackage.solve","text":"function solve(model::MyQLearningModel, environment::T, startstate::Int, maxsteps::Int;\n    Ïµ::Float64 = 0.2) -> MyQLearningModel where T <: AbstractWorldModel\n\nSimulate the Q-Learning agent in the given environment starting from the given state for a maximum number of steps.\n\nArguments\n\nagent::MyQLearningAgentModel: The Q-Learning agent model.\nenvironment::MyRectangularGridWorldModel: The environment model.\nmaxsteps::Int: The maximum number of steps to simulate.\nÎ´::Float64 = 0.02: The convergence threshold. Default is 0.02.\nworldmodel::Function = _world: The world model function. Default is the private _world function.\n\nReturns\n\nMyQLearningAgentModel: The updated Q-Learning agent model after simulation.\n\n\n\n\n\n","category":"method"},{"location":"hopfield/#Hopfield-Networks","page":"Hopfield Networks","title":"Hopfield Networks","text":"Hopfield Networks are a type of recurrent artificial neural network that serve as content-addressable memory systems with binary threshold nodes. They were invented by John Hopfield in 1982 and are used for pattern recognition and associative memory. This was one of the earliest models of what we think of today as a neural network and has influenced the development of more complex architectures, earning Hopfield a place in the history of artificial intelligence and a Nobel Prize of Physics in 2024.\n\nWe've encoded types and methods for both Classical and Modern Hopfield Networks in this package. Below, we provide an overview of how to use these models.","category":"section"},{"location":"hopfield/#VLDataScienceMachineLearningPackage.MyClassicalHopfieldNetworkModel","page":"Hopfield Networks","title":"VLDataScienceMachineLearningPackage.MyClassicalHopfieldNetworkModel","text":"MyClassicalHopfieldNetworkModel <: AbstractlHopfieldNetworkModel\n\nA mutable struct representing a classical Hopfield network model.\n\nFields\n\nW::Array{<:Number, 2}: weight matrix.\nb::Array{<:Number, 1}: bias vector.\nenergy::Dict{Int64, Float32}: energy of the states.\n\n\n\n\n\n","category":"type"},{"location":"hopfield/#VLDataScienceMachineLearningPackage.MyModernHopfieldNetworkModel","page":"Hopfield Networks","title":"VLDataScienceMachineLearningPackage.MyModernHopfieldNetworkModel","text":"MyModernHopfieldNetworkModel <: AbstractlHopfieldNetworkModel\n\nA mutable struct representing a modern Hopfield network model.\n\nFields\n\nX::Array{<:Number, 2}: data matrix with memories stored in the columns.\nXÌ‚::Array{<:Number, 2}: normalized data matrix.\nÎ²::Number: beta parameter (inverse temperature) controlling sharpness of softmax updates.\n\n\n\n\n\n","category":"type"},{"location":"hopfield/#VLDataScienceMachineLearningPackage.recover-Tuple{MyClassicalHopfieldNetworkModel, Vector{Int32}, Float32}","page":"Hopfield Networks","title":"VLDataScienceMachineLearningPackage.recover","text":"recover(model::MyClassicalHopfieldNetworkModel, sâ‚’::Array{Int32,1}, trueenergyvalue::Float32;\n    maxiterations::Int = 1000, patience::Union{Int,Nothing} = nothing,\n    miniterations_before_convergence::Union{Int,Nothing} = nothing) -> Tuple{Dict{Int64, Array{Int32,1}}, Dict{Int64, Float32}}\n\nRun asynchronous Hopfield updates starting from sâ‚’, stopping on convergence, after maxiterations, or once the energy drops below trueenergyvalue. Tracks the state and energy trajectory.\n\nArguments\n\nmodel::MyClassicalHopfieldNetworkModel: Hopfield network parameters.\nsâ‚’::Array{Int32,1}: initial state (Â±1 spins).\ntrueenergyvalue::Float32: early-stopping threshold; iteration halts when current energy is â‰¤ this value.\nmaxiterations::Int: maximum updates before forcing termination.\npatience::Union{Int,Nothing}: buffer length used for equality-based convergence; defaults to max(5, round(Int, 0.1 * N)) with N pixels.\nminiterations_before_convergence::Union{Int,Nothing}: minimum iterations before checking convergence; defaults to patience and is floored at patience.\n\nReturns\n\nTuple of dictionaries keyed from 0:\n\nframes::Dict{Int64, Array{Int32,1}}: spin configuration per iteration.\nenergydictionary::Dict{Int64, Float32}: energy per iteration.\n\n\n\n\n\n","category":"method"},{"location":"hopfield/#VLDataScienceMachineLearningPackage.recover-Union{Tuple{T}, Tuple{MyModernHopfieldNetworkModel, Vector{T}}} where T<:Number","page":"Hopfield Networks","title":"VLDataScienceMachineLearningPackage.recover","text":"recover(model::MyModernHopfieldNetworkModel, sâ‚’::Array{T,1}; \n    maxiterations::Int64 = 1000, Ïµ::Float64 = 1e-10) where T <: Number\n\nIteratively update a modern Hopfield network by alternating softmax probability updates and state reconstruction. Stops when L1 change in probabilities is below Ïµ or after maxiterations.\n\nArguments\n\nmodel::MyModernHopfieldNetworkModel: Hopfield network containing memory matrix X and inverse-temperature Î².\nsâ‚’::Array{T,1}: initial continuous state.\nmaxiterations::Int64: maximum number of update steps (probability + state).\nÏµ::Float64: L1 threshold on successive probability vectors for convergence.\n\nReturns\n\ns::Array{T,1}: final state.\nframes::Dict{Int64, Array{Float32,1}}: stored states per iteration (keyed from 0).\nprobability::Dict{Int64, Array{Float64,1}}: stored probability vectors per iteration (keyed from 0).\n\n\n\n\n\n","category":"method"},{"location":"graphs/#Graphs","page":"Graphs","title":"Graphs","text":"We work with various graph structures in this package, including directed and undirected graphs, as well as specialized graph types for specific applications.","category":"section"},{"location":"graphs/#VLDataScienceMachineLearningPackage.children","page":"Graphs","title":"VLDataScienceMachineLearningPackage.children","text":"function children(graph::T, node::MyGraphNodeModel) -> Set{Int64} where T <: AbstractGraphModel\n\nReturns the set of child node IDs for a given node in the graph.\n\nArguments\n\ngraph::T: The graph to search where T <: AbstractGraphModel.\nnode::MyGraphNodeModel: The node to find children for.\n\nReturns\n\nSet{Int64}: The set of child node IDs.\n\n\n\n\n\n","category":"function"},{"location":"graphs/#VLDataScienceMachineLearningPackage.weight","page":"Graphs","title":"VLDataScienceMachineLearningPackage.weight","text":"function weight(graph::T, source::Int64, target::Int64, edgemodels::Dict{Int64, MyGraphEdgeModel}) -> Any where T <: AbstractGraphModel\n\nReturns the weight of the edge between two nodes in the graph.\n\nArguments\n\ngraph::T: The graph to search where T <: AbstractGraphModel.\nsource::Int64: The ID of the source node.\ntarget::Int64: The ID of the target node.\n\nReturns\n\nAny: The weight of the edge between the source and target nodes. We have this as Any to allow for flexibility in edge weights, which can be of any type.\n\n\n\n\n\nfunction weight(graph::T, source::Int64, target::Int64) -> Float64 where T <: AbstractGraphModel\n\nThis function returns the weight of the edge between two nodes in a graph model.\n\nArguments\n\ngraph::T: the graph model to search. This is a subtype of AbstractGraphModel.\nsource::Int64: the source node id.\ntarget::Int64: the target node id.\n\nReturns\n\nthe weight of the edge between the source and target nodes.\n\n\n\n\n\n","category":"function"},{"location":"graphs/#VLDataScienceMachineLearningPackage.walk","page":"Graphs","title":"VLDataScienceMachineLearningPackage.walk","text":"function walk(graph::T, startnode::MyGraphNodeModel, algorithm::AbstractGraphTraversalAlgorithm; \nverbose::Bool = false) where T <: AbstractGraphModel\n\nThe walk function traverses the graph starting from a given node using the specified algorithm (either Depth-First Search or Breadth-First Search).  It maintains a set of visited nodes to avoid cycles and ensure that each node is processed only once.\n\nArguments\n\ngraph::T: The graph to traverse.\nstartnode::MyGraphNodeModel: The node to start the traversal from.\nalgorithm::AbstractGraphTraversalAlgorithm: The algorithm to use for the traversal. This can be either an instance of DepthFirstSearchAlgorithm or BreadthFirstSearchAlgorithm. Default is BreadthFirstSearchAlgorithm.\nverbose::Bool: Whether to print verbose output (default is false).\n\nReturns\n\nArray{Int64,1}: The collection of visited node IDs in the order they were visited.\n\n\n\n\n\n","category":"function"},{"location":"graphs/#VLDataScienceMachineLearningPackage.findshortestpath","page":"Graphs","title":"VLDataScienceMachineLearningPackage.findshortestpath","text":"findshortestpath(graph::T, start::MyGraphNodeModel; \n    algorithm::AbstractGraphSearchAlgorithm = BellmanFordAlgorithm()) where T <: AbstractGraphModel\n\nThe function computes the shortest paths from a starting node to all other nodes in a graph model. \n\nArguments\n\ngraph::T: the graph model to search. This is a subtype of AbstractGraphModel.\nstart::MyGraphNodeModel: the node to start the search from.\nalgorithm::MyAbstractGraphSearchAlgorithm: the algorithm to use for the search. The default is BellmanFordAlgorithm, but it can also be DijkstraAlgorithm.\n\nReturns\n\na tuple of two dictionaries: the first dictionary contains the distances from the starting node to all other nodes, and the second dictionary contains the previous node in the shortest path from the starting node to all other nodes.\n\n\n\n\n\n","category":"function"},{"location":"graphs/#VLDataScienceMachineLearningPackage.maximumflow","page":"Graphs","title":"VLDataScienceMachineLearningPackage.maximumflow","text":"function maximumflow(graph::T, source::MyGraphNodeModel, sink::MyGraphNodeModel;\n    algorithm::AbstractGraphFlowAlgorithm = FordFulkersonAlgorithm()) where T <: AbstractGraphModel\n\nThis function computes the maximum flow in a directed graph from a source node to a sink node using the specified algorithm. The current implementation does not use the lower bound on the edge capacity (assumes all edges have a lower capacity bound of 0).\n\nArguments\n\ngraph::T: The graph to search. This needs to be a directed graph, with capacities on the edges. \nsource::MyGraphNodeModel: The source node.\nsink::MyGraphNodeModel: The sink node.\nalgorithm::AbstractGraphFlowAlgorithm: The algorithm to use for the search. The default is FordFulkersonAlgorithm. The EdmondsKarpAlgorithm can also be used.\natol::Float64: The absolute tolerance for floating point comparisons (default is 1e-8).\n\nReturns\n\nFloat64: The maximum flow value.\nDict{{Int64,Int64}, Number}: A dictionary mapping each edge (as a tuple of source and target node IDs) to its flow value.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Linear-Systems","page":"Linear Systems","title":"Linear Systems","text":"We provide several techniques for working with linear systems, including eigendecomposition using QR iteration and iterative solvers for linear systems. These solvers are useful for large, sparse systems where direct methods may be inefficient.\n\nIn addition, we provide functionality for solving linear programming problems.","category":"section"},{"location":"solvers/#VLDataScienceMachineLearningPackage.qriteration","page":"Linear Systems","title":"VLDataScienceMachineLearningPackage.qriteration","text":"qriteration(A::Array{Float64,2}; maxiter::Int64 = 10, tolerance::Float64 = 1e-9) -> Tuple\n\nComputes the eigenvalues and eigenvectors of a real matrix A using the QR iteration method.\n\nArguments\n\nA::Array{Float64,2}: a real matrix of size n x n.\nmaxiter::Int64: the maximum number of iterations (default is 10).\ntolerance::Float64: the tolerance for the stopping criterion (default is 1e-9).\n\nReturns\n\nTuple: a tuple of two elements: the first element is an array of eigenvalues and the second element is a dictionary of eigenvectors.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#VLDataScienceMachineLearningPackage.solve","page":"Linear Systems","title":"VLDataScienceMachineLearningPackage.solve","text":"solve(A::AbstractMatrix{T}, b::AbstractVector{T}, xâ‚’::AbstractVector{T}; \nalgorithm::AbstractLinearSolverAlgorithm = JacobiMethod(), Ïµ::Float64 = 0.01, maxiterations::Int64 = 100) where T <: Number\n\nThe solve function solves the linear system of equations Ax = b using the specified algorithm.  The function returns the solution vector x for each iteration of an iterative method. \n\nArguments\n\nA::AbstractMatrix{T}: The system matrix A in the linear system of equations Ax = b.\nb::AbstractVector{T}: The right-hand side vector b in the linear system of equations Ax = b.\nxâ‚’::AbstractVector{T}: The initial guess for the solution vector x.\nalgorithm::AbstractLinearSolverAlgorithm: The algorithm to use to solve the linear system of equations. The default algorithm is JacobiMethod().\nÏµ::Float64: The error tolerance for the iterative method. The default value is 1e-6.\nmaxiterations::Int64: The maximum number of iterations for the iterative method. The default value is 1000.\nÏ‰::Float64: The relaxation factor for the Successive Over-Relaxation method. The default value is 1.0. This parameter is only used if the SuccessiveOverRelaxationMethod algorithm is selected.\n\nReturns\n\nd::Dict{Int,Array{T,1}}: The solution vector x for each iteration of an iterative method. The keys of the dictionary are the iteration numbers, and the values are the solution vectors at each iteration.\n\n\n\n\n\nsolve(problem::MyLinearProgrammingProblemModel) -> Dict{String,Any}\n\nSolves a linear programming problem defined by the MyLinearProgrammingProblemModel instance using the GLPK solver.\n\nArguments\n\nproblem::MyLinearProgrammingProblemModel: An instance of MyLinearProgrammingProblemModel holding the data for the problem.\nconstraints::Symbol: The type of constraints to apply. Options are :leq (less than or equal to), :geq (greater than or equal to), or :eq (equal to). Default is :leq.\n\nReturns\n\nDict{String,Any}: A dictionary with the following keys:\n\"argmax\": The optimal choice.\n\"budget\": The budget at the optimal choice.\n\"objective_value\": The value of the objective function at the optimal choice.\n\n\n\n\n\nsolve(problem::MySimpleCobbDouglasChoiceProblem)\n\nSolve the Cobb-Douglas choice problem and return the results as a dictionary.\n\nArguments\n\nproblem::MySimpleCobbDouglasChoiceProblem: the Cobb-Douglas choice problem\n\nReturns\n\nDict{String,Any}: a dictionary with the results. The dictionary has the following keys:\nargmax::Array{Float64,1}: the optimal choice of goods\nbudget::Float64: the budget used\nobjective_value::Float64: the value of the objective function\n\n\n\n\n\nsolve(model::MyValueIterationModel, problem::MyMDPProblemModel) -> MyValueFunctionPolicy\n\nThis function solves the MDP problem using value iteration.\n\nArguments\n\nmodel::MyValueIterationModel: the value iteration model\nproblem::MyMDPProblemModel: the MDP problem model\n\nReturns\n\nMyValueFunctionPolicy: the value function policy\n\n\n\n\n\nsolve(model::MyRandomRolloutModel, problem::MyMDPProblemModel, \n    world::MyRectangularGridWorldModel, s::Int64) -> Float64\n\nThis function solves the MDP problem using random rollouts.\n\nArguments\n\nmodel::MyRandomRolloutModel: the random rollout model\nproblem::MyMDPProblemModel: the MDP problem model\nworld::MyRectangularGridWorldModel: the world model\ns::Int64: the state\n\nReturns\n\nFloat64: the estimated utility value of the state s\n\n\n\n\n\nsolve(model::AbstractBanditAlgorithmModel; T::Int = 0, world::Function = _null)\n\nSolve the bandit problem using the given model. \n\nArguments\n\nmodel::AbstractBanditAlgorithmModel: The model to use to solve the bandit problem.\nT::Int = 0: The number of rounds to play. Default is 0.\nworld::Function = _null: The function that returns the reward for a given action. Default is the private _null function.\n\nReturns\n\nArray{Float64,2}: The rewards for each arm at each round.\n\n\n\n\n\nsolve(model::AbstractBanditAlgorithmModel, context::AbstractBanditProblemContextModel;\n    T::Int = 0, world::Function = _null)\n\nSolve the contextual bandit problem using the given model and context.\n\nArguments\n\nmodel::MyBinaryVectorArmsEpsilonGreedyAlgorithmModel: The model to use to solve the bandit problem.\ncontext::MyConsumerChoiceBanditContextModel: The context model to use. Must be a subtype of AbstractBanditProblemContextModel.\nT::Int = 0: The number of rounds to play. Default is 0.\nworld::Function = _null: The function that returns the reward for a given action. Default is the private _null function.\nstartdayindex::Int = 1: The starting time index for the context model. Default is 1. This is useful for time series data.\n\n\n\n\n\nfunction solve(model::MyQLearningModel, environment::T, startstate::Int, maxsteps::Int;\n    Ïµ::Float64 = 0.2) -> MyQLearningModel where T <: AbstractWorldModel\n\nSimulate the Q-Learning agent in the given environment starting from the given state for a maximum number of steps.\n\nArguments\n\nagent::MyQLearningAgentModel: The Q-Learning agent model.\nenvironment::MyRectangularGridWorldModel: The environment model.\nmaxsteps::Int: The maximum number of steps to simulate.\nÎ´::Float64 = 0.02: The convergence threshold. Default is 0.02.\nworldmodel::Function = _world: The world model function. Default is the private _world function.\n\nReturns\n\nMyQLearningAgentModel: The updated Q-Learning agent model after simulation.\n\n\n\n\n\n","category":"function"},{"location":".ipynb_checkpoints/index-checkpoint/#VLDataScienceMachineLearningPackage.jl","page":"VLDataScienceMachineLearningPackage.jl","title":"VLDataScienceMachineLearningPackage.jl","text":"The VLDataScienceMachineLearningPackage.jl package is a Julia package that provides functions and types useful for data science, machine learning, and artificial intelligence applications. The package is designed to be simple and easy to use, and it is suitable for students, researchers, and practitioners in the data science and machine learning areas.","category":"section"},{"location":".ipynb_checkpoints/index-checkpoint/#Installation","page":"VLDataScienceMachineLearningPackage.jl","title":"Installation","text":"The package can be installed by running the following command in the Julia REPL:\n\nusing Pkg\nPkg.add(url=\"https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl.git\")","category":"section"},{"location":"mdp/#Markov-Decision-Processes-(MDPs)","page":"Markov Decision Processes","title":"Markov Decision Processes (MDPs)","text":"We've developed some codes to work with Markov Decision Processes (MDPs). MDPs are mathematical frameworks used for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. They are widely used in various fields, including robotics, economics, and artificial intelligence, particularly in reinforcement learning.","category":"section"},{"location":"mdp/#Types","page":"Markov Decision Processes","title":"Types","text":"We have defined the following abstract and concrete types to represent MDPs and related concepts:\n\nWe construct the MyMDProblemModel type, and the MyRectangularGridWorldModel type to represent the environment in which the MDP operates using custom build methods. The MyValueIterationModel, MyRandomRolloutModel, and MyValueFunctionPolicy can be constructed using their default constructors.","category":"section"},{"location":"mdp/#Value-Iteration-Algorithm","page":"Markov Decision Processes","title":"Value Iteration Algorithm","text":"We have implemented the Value Iteration algorithm, which is a dynamic programming algorithm used to compute the optimal policy and value function for an MDP. The algorithm iteratively updates the value of each state based on the expected rewards and the values of successor states.","category":"section"},{"location":"mdp/#Cobb-Douglas-Choice-Problem","page":"Markov Decision Processes","title":"Cobb-Douglas Choice Problem","text":"We have also implemented a simple Cobb-Douglas choice problem, which is a type of economic model used to represent consumer preferences and choices. The Cobb-Douglas utility function is commonly used in economics to model the relationship between consumption of goods and overall utility.","category":"section"},{"location":"mdp/#VLDataScienceMachineLearningPackage.MyMDPProblemModel","page":"Markov Decision Processes","title":"VLDataScienceMachineLearningPackage.MyMDPProblemModel","text":"mutable struct MyMDPProblemModel <: AbstractProcessModel\n\nA mutable struct that defines a Markov Decision Process (MDP) model.  The MDP model is defined by the tuple (ð’®, ð’œ, T, R, Î³).  The state space ð’® is an array of integers, the action space ð’œ is an array of integers,  the transition matrix T is a function or a 3D array, the reward matrix R is a function or a 2D array,  and the discount factor Î³ is a float.\n\nFields\n\nð’®::Array{Int64,1}: state space\nð’œ::Array{Int64,1}: action space\nT::Union{Function, Array{Float64,3}}: transition matrix of function\nR::Union{Function, Array{Float64,2}}: reward matrix or function\nÎ³::Float64: discount factor\n\n\n\n\n\n","category":"type"},{"location":"mdp/#VLDataScienceMachineLearningPackage.MyRectangularGridWorldModel","page":"Markov Decision Processes","title":"VLDataScienceMachineLearningPackage.MyRectangularGridWorldModel","text":"mutable struct MyRectangularGridWorldModel <: AbstractWorldModel\n\nA mutable struct that defines a rectangular grid world model.\n\nFields\n\nnumber_of_rows::Int: number of rows in the grid\nnumber_of_cols::Int: number of columns in the grid\ncoordinates::Dict{Int,Tuple{Int,Int}}: dictionary of state to coordinate mapping\nstates::Dict{Tuple{Int,Int},Int}: dictionary of coordinate to state mapping\nmoves::Dict{Int,Tuple{Int,Int}}: dictionary of state to move mapping\nrewards::Dict{Int,Float64}: dictionary of state to reward mapping\n\n\n\n\n\n","category":"type"},{"location":"mdp/#VLDataScienceMachineLearningPackage.MyValueIterationModel","page":"Markov Decision Processes","title":"VLDataScienceMachineLearningPackage.MyValueIterationModel","text":"struct MyValueIterationModel <: AbstractProcessModel\n\nA struct that defines a value iteration model.  The value iteration model is defined by the maximum number of iterations k_max.\n\n\n\n\n\n","category":"type"},{"location":"mdp/#VLDataScienceMachineLearningPackage.MyValueFunctionPolicy","page":"Markov Decision Processes","title":"VLDataScienceMachineLearningPackage.MyValueFunctionPolicy","text":"struct MyValueFunctionPolicy\n\nA struct that defines a value function policy.\n\nFields\n\nproblem::MyMDPProblemModel: MDP problem model\nU::Array{Float64,1}: value function vector. This holds the Utility of each state.\n\n\n\n\n\n","category":"type"},{"location":"mdp/#VLDataScienceMachineLearningPackage.MyRandomRolloutModel","page":"Markov Decision Processes","title":"VLDataScienceMachineLearningPackage.MyRandomRolloutModel","text":"struct MyRandomRolloutModel\n\nA struct that defines a random rollout model. The random rollout model is defined by the depth of the rollout.\n\n\n\n\n\n","category":"type"},{"location":"mdp/#VLDataScienceMachineLearningPackage.solve-Tuple{MyValueIterationModel, MyMDPProblemModel}","page":"Markov Decision Processes","title":"VLDataScienceMachineLearningPackage.solve","text":"solve(model::MyValueIterationModel, problem::MyMDPProblemModel) -> MyValueFunctionPolicy\n\nThis function solves the MDP problem using value iteration.\n\nArguments\n\nmodel::MyValueIterationModel: the value iteration model\nproblem::MyMDPProblemModel: the MDP problem model\n\nReturns\n\nMyValueFunctionPolicy: the value function policy\n\n\n\n\n\n","category":"method"},{"location":"mdp/#VLDataScienceMachineLearningPackage.backup-Tuple{MyMDPProblemModel, Vector{Float64}, Int64}","page":"Markov Decision Processes","title":"VLDataScienceMachineLearningPackage.backup","text":"backup(problem::MyMDPProblemModel, U::Array{Float64,1}, s::Int64) -> Float64\n\nThis function computes the backup value for a given state s and value function U.\n\nArguments\n\nproblem::MyMDPProblemModel: the MDP problem model\nU::Array{Float64,1}: the value function vector\ns::Int64: the state\n\nReturns\n\nFloat64: the best backup value for the state s\n\n\n\n\n\n","category":"method"},{"location":"mdp/#VLDataScienceMachineLearningPackage.policy-Tuple{Matrix{Float64}}","page":"Markov Decision Processes","title":"VLDataScienceMachineLearningPackage.policy","text":"policy(Q_array::Array{Float64,2}) -> Array{Int,1}\n\nThis function computes the policy from the Q-value function.\n\nArguments\n\nQ_array::Array{Float64,2}: the Q-value function\n\nReturns\n\nArray{Int,1}: the policy which maps states to actions\n\n\n\n\n\n","category":"method"},{"location":"mdp/#VLDataScienceMachineLearningPackage.Q-Tuple{MyMDPProblemModel, Vector{Float64}}","page":"Markov Decision Processes","title":"VLDataScienceMachineLearningPackage.Q","text":"Q(p::MyMDPProblemModel, U::Array{Float64,1}) -> Array{Float64,2}\n\nThis function computes the Q-value function for a given value function U.\n\nArguments\n\np::MyMDPProblemModel: the MDP problem model\nU::Array{Float64,1}: the value function vector\n\nReturns\n\nArray{Float64,2}: the Q-value function\n\n\n\n\n\n","category":"method"},{"location":"mdp/#VLDataScienceMachineLearningPackage.lookahead","page":"Markov Decision Processes","title":"VLDataScienceMachineLearningPackage.lookahead","text":"lookahead(p::MyMDPProblemModel, U::Vector{Float64}, s::Int64, a::Int64)\n\nThis function computes the lookahead value for a given state-action pair (s,a).  It uses a vector U to compute the value function.\n\nArguments\n\np::MyMDPProblemModel: the MDP problem model\nU::Vector{Float64}: the value function vector\ns::Int64: the state\na::Int64: the action\n\nReturns\n\nFloat64: the lookahead value for the state-action pair (s,a). \n\n\n\n\n\nlookahead(p::MyMDPProblemModel, U::Function, s::Int64, a::Int64)::Float64\n\nThis function computes the lookahead value for a given state-action pair (s,a).  It uses a function U to compute the value function.\n\n\n\n\n\n","category":"function"},{"location":"mdp/#VLDataScienceMachineLearningPackage.iterative_policy_evaluation","page":"Markov Decision Processes","title":"VLDataScienceMachineLearningPackage.iterative_policy_evaluation","text":"iterative_policy_evaluation(p::MyMDPProblemModel, Ï€, k_max::Int) -> Array{Float64,1}\n\nThis function performs iterative policy evaluation for a given MDP problem and policy.\n\nArguments\n\np::MyMDPProblemModel: the MDP problem model\nÏ€: the policy function\nk_max::Int: the maximum number of iterations\n\n\n\n\n\n","category":"function"},{"location":"mdp/#VLDataScienceMachineLearningPackage.greedy","page":"Markov Decision Processes","title":"VLDataScienceMachineLearningPackage.greedy","text":"greedy(problem::MyMDPProblemModel, U::Array{Float64,1}, s::Int64) -> (a::Int64, u::Float64)\n\nThis function computes the greedy action and its value for a given state s and value function U.\n\nArguments\n\nproblem::MyMDPProblemModel: the MDP problem model\nU::Array{Float64,1}: the value function vector\ns::Int64: the state\n\nReturns\n\n(a::Int64, u::Float64): a tuple of the best action and its value\n\n\n\n\n\n","category":"function"},{"location":"mdp/#VLDataScienceMachineLearningPackage.myrandpolicy-Tuple{MyMDPProblemModel, MyRectangularGridWorldModel, Int64}","page":"Markov Decision Processes","title":"VLDataScienceMachineLearningPackage.myrandpolicy","text":"myrandpolicy(problem::MyMDPProblemModel, world::MyRectangularGridWorldModel, s::Int)::Int\n\nThis function implements a random policy for a given MDP problem and world model.\n\nArguments\n\nproblem::MyMDPProblemModel: the MDP problem model\nworld::MyRectangularGridWorldModel: the world model\ns::Int: the state\n\nReturns\n\nInt: the action we choose\n\n\n\n\n\n","category":"method"},{"location":"mdp/#VLDataScienceMachineLearningPackage.myrandstep-Tuple{MyMDPProblemModel, MyRectangularGridWorldModel, Int64, Int64}","page":"Markov Decision Processes","title":"VLDataScienceMachineLearningPackage.myrandstep","text":"myrandstep(problem::MyMDPProblemModel, world::MyRectangularGridWorldModel, s::Int, a::Int)\n\nThis function implements a random step for a given MDP problem and world model.\n\nArguments\n\nproblem::MyMDPProblemModel: the MDP problem model\nworld::MyRectangularGridWorldModel: the world model\ns::Int: the state we are in\na::Int: the action we choose\n\nReturns\n\n(sâ€²,r): a tuple of the next state and the reward for being in state s and taking action a\n\n\n\n\n\n","category":"method"},{"location":"mdp/#VLDataScienceMachineLearningPackage.solve-Tuple{MyRandomRolloutModel, MyMDPProblemModel, MyRectangularGridWorldModel, Int64}","page":"Markov Decision Processes","title":"VLDataScienceMachineLearningPackage.solve","text":"solve(model::MyRandomRolloutModel, problem::MyMDPProblemModel, \n    world::MyRectangularGridWorldModel, s::Int64) -> Float64\n\nThis function solves the MDP problem using random rollouts.\n\nArguments\n\nmodel::MyRandomRolloutModel: the random rollout model\nproblem::MyMDPProblemModel: the MDP problem model\nworld::MyRectangularGridWorldModel: the world model\ns::Int64: the state\n\nReturns\n\nFloat64: the estimated utility value of the state s\n\n\n\n\n\n","category":"method"},{"location":"mdp/#VLDataScienceMachineLearningPackage.MySimpleCobbDouglasChoiceProblem","page":"Markov Decision Processes","title":"VLDataScienceMachineLearningPackage.MySimpleCobbDouglasChoiceProblem","text":"mutable struct MySimpleCobbDouglasChoiceProblem\n\nA model for a Cobb-Douglas choice problem. \n\nFields\n\nÎ±::Array{Float64,1}: the vector of parameters for the Cobb-Douglas utility function (preferences)\nc::Array{Float64,1}: the vector of unit prices for the goods\nI::Float64: the income the consumer has to spend\nbounds::Array{Float64,2}: the bounds on the goods [0,U] where U is the upper bound\ninitial::Array{Float64,1}: the initial guess for the solution\n\n\n\n\n\n","category":"type"},{"location":"mdp/#VLDataScienceMachineLearningPackage.solve-Tuple{MySimpleCobbDouglasChoiceProblem}","page":"Markov Decision Processes","title":"VLDataScienceMachineLearningPackage.solve","text":"solve(problem::MySimpleCobbDouglasChoiceProblem)\n\nSolve the Cobb-Douglas choice problem and return the results as a dictionary.\n\nArguments\n\nproblem::MySimpleCobbDouglasChoiceProblem: the Cobb-Douglas choice problem\n\nReturns\n\nDict{String,Any}: a dictionary with the results. The dictionary has the following keys:\nargmax::Array{Float64,1}: the optimal choice of goods\nbudget::Float64: the budget used\nobjective_value::Float64: the value of the objective function\n\n\n\n\n\n","category":"method"},{"location":"text/#Working-with-Text-Data","page":"Text","title":"Working with Text Data","text":"We'll work with text data in many applications. We've included a few methods to help with text processing. ","category":"section"},{"location":"text/#English-Language-Vocabulary-Model","page":"Text","title":"English Language Vocabulary Model","text":"We have included a simple vocabulary model that we can use to analyze text data. This model is based on a  dictionary of common English words. We can use this model to compute a transition matrix that describes the  probability of transitioning from one letter to another in a word. This can be useful for various applications,  including text generation and analysis.","category":"section"},{"location":"text/#VLDataScienceMachineLearningPackage.tokenize","page":"Text","title":"VLDataScienceMachineLearningPackage.tokenize","text":"function tokenize(s::String, tokens::Dict{String, Int64}; \n    pad::Int64 = 0, padleft::Bool = false, delim::Char = ' ') -> Array{Int64,1}\n\nArguments\n\ns::String - the string to tokenize.\ntokens::Dict{String, Int64} - a dictionary of tokens in alphabetical order (key: token, value: position) for the entire document.\npad::Int64 - (optional) the number of padding tokens to add to the end of the tokenized string. Default is 0.\npadleft::Bool - (optional) if true, the padding tokens are added to the beginning of the tokenized string. Default is false.\ndelim::Char - (optional) the delimiter used in the string. Default is ' '.\n\nReturns\n\nArray{Int64,1} - an array of integers representing the vectorized string.\n\n\n\n\n\n","category":"function"},{"location":"text/#VLDataScienceMachineLearningPackage.featurehashing","page":"Text","title":"VLDataScienceMachineLearningPackage.featurehashing","text":"function featurehashing(text::Array{String,1}; d::Int64 = 100, \n    algorithm::AbstractFeatureHashingAlgorithm = UnsignedFeatureHasing()) -> Array{Int64,1}\n\nComputes the feature hashing of the input text using the specified algorithm.\n\nArguments\n\ntext::Array{String,1} - an array of strings to be hashed.\nd::Int64 - (optional) the size of the hash table. Default is 100.\nalgorithm::AbstractFeatureHasingAlgorithm - (optional) the hashing algorithm to use. Default is UnsignedFeatureHasing.\n\nReturns\n\nArray{Int64,1} - an array of integers representing the hashed features.\n\n\n\n\n\nfunction featurehashing(text::Array{Int,1}; d::Int64 = 100, \n    algorithm::AbstractFeatureHashingAlgorithm = UnsignedFeatureHasing()) -> Array{Int64,1}\n\nComputes the feature hashing of the input text using the specified algorithm.\n\nArguments\n\ntext::Array{Int,1} - an array of integers to be hashed (e.g., tokenized text).\nd::Int64 - (optional) the size of the hash table. Default is 100.\nalgorithm::AbstractFeatureHasingAlgorithm - (optional) the hashing algorithm to use. Default is UnsignedFeatureHasing.\n\nReturns\n\nArray{Int64,1} - an array of integers representing the hashed features.\n\n\n\n\n\n","category":"function"},{"location":"text/#VLDataScienceMachineLearningPackage.vocabulary_transition_matrix","page":"Text","title":"VLDataScienceMachineLearningPackage.vocabulary_transition_matrix","text":"vocabulary_transition_matrix(wordsdictionary::Dict{Char, Set{String}}, \n    letters::Array{Char,1}) -> Array{Float64,2}\n\nThis function computes the transition matrix for {a,b,c,...,z} given the vocabulary model and the letters.\n\nArguments\n\nwordsdictionary::Dict{Char, Set{String}}: This is the vocabulary model that we have created.\nletters::Array{Char,1}: This is the array of characters that we are interested in.\n\nReturns\n\nArray{Float64,2}: This returns back the transition matrix.\n\n\n\n\n\n","category":"function"},{"location":"text/#VLDataScienceMachineLearningPackage.sample_words","page":"Text","title":"VLDataScienceMachineLearningPackage.sample_words","text":"sample_words(P::Array{Float64,2}, letters::Array{Char,1};\n    number_of_samples::Int64 = 100, length_of_sample_word::Int64 = 4, startchar::Char = 'z') -> Dict{Int64,String}\n\nThis function generates sample words of a specified length from the transition matrix P given the letters.\n\nArguments\n\nP::Array{Float64,2}: This is the transition matrix.\nletters::Array{Char,1}: This is the array of characters that we are interested in.\nnumber_of_samples::Int64: This is the number of samples that we want to generate.\nlength_of_sample_word::Int64: This is the length of the sample word that we want to generate.\nstartchar::Char: This is the starting character of the sample word.\n\nReturns\n\nDict{Int64,String}: This returns back a dictionary of sample words. The key is the index of the sample word and the value is the sample word.\n\n\n\n\n\n","category":"function"},{"location":"online/#Multiplicative-Weight-Updates","page":"Multiplicative Weight Updates","title":"Multiplicative Weight Updates","text":"This section describes the implementation of the Binary Weighted Majority Algorithm (WMA) and Multiplicative Weights Algorithm (MWA) for online learning in the VLDataScienceMachineLearningPackage.jl Julia package.","category":"section"},{"location":"online/#Zero-Sum-Game-Setting","page":"Multiplicative Weight Updates","title":"Zero-Sum Game Setting","text":"Let's consider the application of the multiplicative weights update algorithm to zero-sum games. \n\nIn a zero-sum game, players have opposing interests, and the players' payoffs sum to zero: one's gain is the other's loss. The multiplicative-weights (MW) algorithm finds (approximate) Nash equilibria by down-weighting poorly performing actions over repeated play.\n\nLet's dig into some the details of the game:\n\nGame: Consider a competitive setting with k players. A game is called zero-sum if, for any outcome, the players' payoffs add to zero. The standard theory we use below focuses on the k = 2 case. Each player chooses an action a in mathcalA from some finite action set mathcalA with mathcalA = N. For the two-player case, we model payoffs with a matrix mathbfM in mathbbR^N times N (for simplicity, assume both players have N actions). If the row player chooses action i and the column player chooses action j, then the row player's payoff is m_ij and the column player's payoff is -m_ij. This is what we mean by zero-sum: whatever one player gains, the other loses.\nGoals: The row player wants to maximize their payoff. The column player wants to minimize the row player's payoff. Let the row player randomize over rows using a mixed strategy mathbfp (a probability distribution over the N rows), and let the column player randomize over columns using a mixed strategy mathbfq (a probability distribution over the N columns). The expected payoff to the row player is mathbfp^topmathbfMmathbfq and because the game is zero-sum, the expected payoff to the column player is -mathbfp^topmathbfMmathbfq. So both players care about the same scalar mathbfp^topmathbfMmathbfq, but they pull it in opposite directions.\nNash Equilibrium: A Nash equilibrium is a pair of (possibly mixed) strategies (mathbfp^* mathbfq^*) such that each player's strategy is a best response to the other's. In other words, given mathbfq^*, the row player cannot switch from mathbfp^* to some other mathbfp and improve their expected payoff, and given mathbfp^*, the column player cannot switch from mathbfq^* to some other mathbfq and further reduce the row player's expected payoff.\n\nIn a two-player zero-sum game, every Nash equilibrium corresponds to a minimax solution. The minimax theorem guarantees that: $ \\max{\\mathbf{p}} \\min{\\mathbf{q}} \\mathbf{p}^{\\top}\\mathbf{M}\\mathbf{q} = \\min{\\mathbf{q}} \\max{\\mathbf{p}} \\mathbf{p}^{\\top}\\mathbf{M}\\mathbf{q} = v $ where v is called the value of the game. At equilibrium, the row player's strategy mathbfp^* guarantees at least v no matter what the column player does, and the column player's strategy mathbfq^* holds the row player to at most v no matter what the row player does. That shared value v is the Nash equilibrium payoff.\n\nFinally, learning dynamics: if both players repeatedly play the game and update their mixed strategies using sublinear algorithms such as multiplicative weights, then the time-averaged strategies approach an epsilon-Nash equilibrium (equivalently, an epsilon-minimax solution), where epsilon becomes small as regret becomes small.","category":"section"},{"location":"online/#Algorithm","page":"Multiplicative Weight Updates","title":"Algorithm","text":"Let's outline a simple implementation of the multiplicative weights update algorithm for a two-player zero-sum game. Given a payoff matrix mathbfMinmathbbR^NtimesN, we want to find a mixed strategy, a probability distribution over actions, for the row player that minimizes expected loss.\n\nInitialization: Given a payoff matrix mathbfMinmathbbR^NtimesN, where the payoffs (elements of mathbfM) are in the range m_ijin-1 1.  Initialize the weights w_i^(1) gets 1 for all actions iinmathcalA, where mathcalA = 12dotsN, and set the learning rate etain(01).\n\nChoosing T: The number of rounds T determines the accuracy of the approximate Nash equilibrium. To achieve an epsilon-Nash equilibrium, choose T geq fracln Nepsilon^2. For example, with N=10 actions and desired accuracy epsilon=01, we need T geq fracln 10001 approx 230 rounds.\n\nChoosing Î·: The learning rate eta controls the step size of weight updates. Common rules of thumb include:Theory-based: eta = sqrtfracln NT optimizes the convergence bound\nSimple rule: eta = frac1sqrtT for practical applications  \nAdaptive: Start with eta = 01 and reduce by half if convergence stalls\nConstraint: Ensure eta leq 1 to prevent negative weights (since losses are bounded in -11)\n\nFor each round t=12dotsT do:\n\nCompute the normalization factor: Phi^(t) gets sum_i=1^Nw_i^(t).\nRow player computes its strategy: The row player will choose an action with probability mathbfp^(t) gets leftw_i^(t)Phi^(t) mid i = 12dotsNright. Let the row player action be i^star.\nColumn player computes its strategy: The column player will choose action: jgets textargmin_jinmathcalAleftmathbfp^(t)topmathbfMmathbfe_jright, so that mathbfq^(t) gets mathbfe_j, where mathbfe_j is the j-th standard basis vector. The row player experiences loss vector boldsymbolell^(t) gets mathbfLmathbfq^(t), where mathbfL = -mathbfM is the loss matrix.\nUpdate the weights: w_i^(t+1) gets w_i^(t)expbigl(-etaell_i^(t)bigr) for all actions iinmathcalA for the row player.","category":"section"},{"location":"online/#Convergence","page":"Multiplicative Weight Updates","title":"Convergence","text":"After T rounds, define the average strategies:   $ \\bar p \\;=\\;\\frac{1}{T}\\sum{t=1}^{T}p^{(t)},  \\quad \\bar q \\;=\\;\\frac{1}{T}\\sum{t=1}^{T}q^{(t)}. $ Then (bar pbar q) is an epsilon-Nash equilibrium with $   \\max{q}\\,\\bar p^\\top M\\,q   \\;-\\;\\min{p}\\,p^\\top M\\,\\bar q   \\;\\le\\;\\epsilon,   \\quad   \\epsilon = O\\Bigl(\\sqrt{\\tfrac{\\ln N}{T}}\\Bigr). $","category":"section"},{"location":"online/#VLDataScienceMachineLearningPackage.MyBinaryWeightedMajorityAlgorithmModel","page":"Multiplicative Weight Updates","title":"VLDataScienceMachineLearningPackage.MyBinaryWeightedMajorityAlgorithmModel","text":"MyBinaryWeightedMajorityAlgorithmModel\n\nA mutable type for the Binary Weighted Majority Algorithm model.  This model is used to simulate the Binary Weighted Majority Algorithm. The model has the following fields:\n\nÏµ::Float64: learning rate\nn::Int64: number of experts\nT::Int64: number of rounds\nweights::Array{Float64,2}: weights of the experts\nexpert::Function: expert function\nadversary::Function: adversary function\n\n\n\n\n\n","category":"type"},{"location":"online/#VLDataScienceMachineLearningPackage.play","page":"Multiplicative Weight Updates","title":"VLDataScienceMachineLearningPackage.play","text":"play(model::MyBinaryWeightedMajorityAlgorithmModel, data::Array{Float64,2})\n\nPlay the Binary Weighted Majority Algorithm. This function simulates the Binary Weighted Majority Algorithm using the given model and data. The function returns a tuple with two elements. The first element is a matrix with the results of the simulation. The second element is the weights of the experts at the end of the simulation.\n\nArguments\n\nmodel::MyBinaryWeightedMajorityAlgorithmModel: the model to simulate\nworld::Function: the world function that generates the actual outcomes\ndata::Array{Float64,2}: the data to use in the simulation\n\nReturns\n\nTuple{Array{Int64,2}, Array{Float64,2}}: a tuple with the results of the simulation and the weights of the experts\n\n\n\n\n\nfunction play(model::MyTwoPersonZeroSumGameModel)::Tuple{Array{Int64,2}, Array{Float64,2}}\n\nThis method plays the two-person zero-sum game using the MyTwoPersonZeroSumGameModel instance.  It returns the results of the game and the updated weights of the experts.\n\nArguments\n\nmodel::MyTwoPersonZeroSumGameModel: An instance of the MyTwoPersonZeroSumGameModel type.\n\nReturns\n\nresults_array::Array{Int64,2}: A 2D array containing the results of the game. Each row corresponds to a round, and the columns contain:\nThe first column is the action of the row player (aggregator).\nThe second column is the action of the column player (adversary).\nweights::Array{Float64,2}: A 2D array containing the updated weights of the experts after each round.\n\n\n\n\n\n","category":"function"},{"location":"online/#VLDataScienceMachineLearningPackage.MyTwoPersonZeroSumGameModel","page":"Multiplicative Weight Updates","title":"VLDataScienceMachineLearningPackage.MyTwoPersonZeroSumGameModel","text":"mutable struct MyTwoPersonZeroSumGameModel <: AbstractOnlineLearningModel\n\nA mutable type for the Two-Person Zero-Sum Game model.  This model is used to simulate the Two-Person Zero-Sum Game using the Multiplicative Weights Algorithm.  The model has the following fields:\n\nÏµ::Float64: learning rate\nn::Int64: number of experts (actions)\nT::Int64: number of rounds\nweights::Array{Float64,2}: weights of the experts\npayoffmatrix::Array{Float64,2}: payoff matrix\n\n\n\n\n\n","category":"type"},{"location":"online/#VLDataScienceMachineLearningPackage.play-Tuple{MyTwoPersonZeroSumGameModel}","page":"Multiplicative Weight Updates","title":"VLDataScienceMachineLearningPackage.play","text":"function play(model::MyTwoPersonZeroSumGameModel)::Tuple{Array{Int64,2}, Array{Float64,2}}\n\nThis method plays the two-person zero-sum game using the MyTwoPersonZeroSumGameModel instance.  It returns the results of the game and the updated weights of the experts.\n\nArguments\n\nmodel::MyTwoPersonZeroSumGameModel: An instance of the MyTwoPersonZeroSumGameModel type.\n\nReturns\n\nresults_array::Array{Int64,2}: A 2D array containing the results of the game. Each row corresponds to a round, and the columns contain:\nThe first column is the action of the row player (aggregator).\nThe second column is the action of the column player (adversary).\nweights::Array{Float64,2}: A 2D array containing the updated weights of the experts after each round.\n\n\n\n\n\n","category":"method"},{"location":"factory/#Factory-methods","page":"Factory","title":"Factory methods","text":"We use a particular pattern to build and configure the various composite types in our system. This pattern involves the use of factory functions that encapsulate the construction logic for each type, ensuring that all necessary dependencies are properly initialized and configured.","category":"section"},{"location":"factory/#VLDataScienceMachineLearningPackage.build","page":"Factory","title":"VLDataScienceMachineLearningPackage.build","text":"function build(modeltype::Type{T}, \n    data::NamedTuple)::T where T <: AbstractLinearProgrammingProblemType\n\nThe function builds a linear programming problem model from the data provided.\n\nArguments\n\nmodeltype::Type{T}: the type of the model to build where T is a subtype of AbstractLinearProgrammingProblemType.\ndata::NamedTuple: the data to use to build the model.   \n\nThe data::NamedTuple must have the following fields:\n\nA::Array{Float64,2}: the constraint matrix.\nb::Array{Float64,1}: the right-hand side vector.\nc::Array{Float64,1}: the cost vector.\nlb::Array{Float64,1}: the lower bounds vector.\nub::Array{Float64,1}: the upper bounds vector.\n\nReturns\n\na linear programming problem model of type T where T is a subtype of AbstractLinearProgrammingProblemType.\n\n\n\n\n\nbuild(modeltype::Type{MyPerceptronClassificationModel}, \n    data::NamedTuple) -> MyPerceptronClassificationModel\n\nThe function builds a perceptron classification model from the data provided.\n\nArguments\n\nmodeltype::Type{MyPerceptronClassificationModel}: the type of the model to build.\ndata::NamedTuple: the data to use to build the model.\n\nThe data::NamedTuple must have the following fields:\n\nparameters::Vector{Float64}: the coefficients of the model.\nmistakes::Int64: the number of mistakes that are are willing to make.\n\nReturns\n\na perceptron classification model.\n\n\n\n\n\nbuild(modeltype::Type{MyLogisticRegressionClassificationModel}, \n    data::NamedTuple) -> MyLogisticRegressionClassificationModel\n\nThe function builds a logistic regression classification model from the data provided.\n\nArguments\n\nmodeltype::Type{MyLogisticRegressionClassificationModel}: the type of the model to build.\ndata::NamedTuple: the data to use to build the model.\n\nThe data::NamedTuple must have the following fields:\n\nparameters::Vector{Float64}: the coefficients of the model.\n\nReturns\n\na logistic regression classification model.\n\n\n\n\n\nbuild(modeltype::Type{MyKNNClassificationModel}, \n    data::NamedTuple) -> MyKNNClassificationModel\n\nThe function builds a KNN classification model from the data provided.\n\nArguments\n\nmodeltype::Type{MyKNNClassificationModel}: the type of the model to build.\ndata::NamedTuple: the data to use to build the model.\n\nThe data::NamedTuple must have the following fields:\n\nK::Int64: the number of neighbours to look at.\nd::Function: the similarity function.\nX::Matrix{Float64}: the training features.\ny::Vector{Int64}: the training labels.\n\n\n\n\n\nfunction build(type::Type{MyAdjacencyRecombiningCommodityPriceTree}, data::NamedTuple) -> MyAdjacencyRecombiningCommodityPriceTree\n\nBuilds an MyAdjacencyRecombiningCommodityPriceTree model given the data in the NamedTuple.  This method builds the connectivity of the tree. To compute the price at each node, use the populate! method.\n\nArguments\n\ntype::Type{MyAdjacencyRecombiningCommodityPriceTree}: The type of the model to build.\ndata::NamedTuple: The data to use to build the model.\n\nThe data NamedTuple must contain the following fields:\n\nh::Int64: The height of the tree.\nprice::Float64: The price at the root node.\nu::Float64: The price increase factor.\nd::Float64: The price decrease factor.\n\nReturns\n\nMyAdjacencyRecombiningCommodityPriceTree: the price tree model holding the computed price data.\n\n\n\n\n\nfunction build(modeltype::Type{MyFullGeneralAdjacencyTree}, data::NamedTuple) -> MyFullGeneralAdjacencyTree\n\nThis function builds a MyFullGeneralAdjacencyTree model given the data in the NamedTuple.  It populates the connectivity of the tree. However, it does not populate the data for the tree nodes. We populate the data using the populate! method.\n\nArguments\n\nmodeltype::Type{MyFullGeneralAdjacencyTree}: The type of the model to build.\ndata::NamedTuple: The data to use to build the model. The NamedTuple must have the following fields:\nh::Int64: The height of the tree.\nn::Int64: The branching factor of the tree.\n\nReturns\n\nMyFullGeneralAdjacencyTree: The constructed tree model.\n\n\n\n\n\nfunction build(modeltype::Type{MyOneDimensionalElementaryWolframRuleModel}, data::NamedTuple) -> MyOneDimensionalElementarWolframRuleModel\n\nThis build method constructs an instance of the MyOneDimensionalElementaryWolframRuleModel type using the data in a NamedTuple.\n\nArguments\n\nmodeltype::Type{MyOneDimensionalElementaryWolframRuleModel}: The type of model to build, in this case, the MyOneDimensionalElementaryWolframRuleModel type.\ndata::NamedTuple: The data to use to build the model.\n\nThe data::NamedTuple must contain the following keys:\n\nindex::Int64: The index of the Wolfram rule\ncolors::Int64: The number of colors in the rule\nradius::Int64: The radius, i.e., the number of cells to consider in the rule\n\nReturn\n\nThis function returns a populated instance of the MyOneDimensionalElementaryWolframRuleModel type.\n\n\n\n\n\nfunction build(model::Type{T}, edgemodels::Dict{Int64, MyGraphEdgeModel}) where T <: AbstractGraphModel\n\nThis function builds a graph model from a dictionary of edge models.\n\nArguments\n\nmodel::Type{T}: The type of graph model to build, where T is a subtype of AbstractGraphModel.\nedgemodels::Dict{Int64, MyGraphEdgeModel}: A dictionary of edge models to use for building the graph.\n\nReturns\n\nT: The constructed graph model, where T is a subtype of AbstractGraphModel.\n\n\n\n\n\nfunction build(modeltype::Type{MyDirectedBipartiteGraphModel}, data::NamedTuple) -> MyDirectedBipartiteGraphModel\n\nThis function builds a mutable MyDirectedBipartiteGraphModel instance given the data in the data::NamedTuple argument.\n\nArguments\n\nmodeltype::Type{MyDirectedBipartiteGraphModel} - The type of the model to build.\ndata::NamedTuple - The data to populate the model with.\n\nThe data::NamedTuple argument must contain the following fields:\n\ns::Int64 - The source node index.\nt::Int64 - The target node index.\nedges::Dict{Int, MyConstrainedGraphEdgeModel} - The edges dictionary containing the constrained graph edges models.\n\n\n\n\n\nbuild(model::Type{MyMDPProblemModel}, data::NamedTuple) -> MyMDPProblemModel\n\nBuilds a MyMDPProblemModel from a NamedTuple.\n\nArguments\n\nmodel::Type{MyMDPProblemModel}: the model type to build\ndata::NamedTuple: the data to use to build the model\n\nThe data NamedTuple must contain the following keys:\n\nð’®::Array{Int64,1}: state space\nð’œ::Array{Int64,1}: action space\nT::Union{Function, Array{Float64,3}}: transition matrix of function\nR::Union{Function, Array{Float64,2}}: reward matrix or function\nÎ³::Float64: discount factor\n\nReturns\n\nMyMDPProblemModel: the built MDP problem model\n\n\n\n\n\nbuild(modeltype::Type{MyRectangularGridWorldModel}, data::NamedTuple) -> MyRectangularGridWorldModel\n\nBuilds a MyRectangularGridWorldModel from data in a NamedTuple.\n\nArguments\n\nmodeltype::Type{MyRectangularGridWorldModel}: the model type to build\ndata::NamedTuple: the data to use to build the model\n\nThe data NamedTuple must contain the following keys:\n\nnrows::Int: number of rows in the grid\nncols::Int: number of columns in the grid\nrewards::Dict{Tuple{Int,Int},Float64}: dictionary of state to reward mapping\ndefaultreward::Float64: default reward value (optional)\n\nReturns\n\nMyRectangularGridWorldModel: a populated rectangular grid world model\n\n\n\n\n\nbuild(type::MySimpleCobbDouglasChoiceProblem, data::NamedTuple) -> MySimpleCobbDouglasChoiceProblem\n\nBuilds a MySimpleCobbDouglasChoiceProblem model from data in a NamedTuple.\n\nArguments\n\ntype::MySimpleCobbDouglasChoiceProblem: the type of model to build\ndata::NamedTuple: the data to use to build the model\n\nThe data NamedTuple must contain the following keys:\n\nÎ±::Array{Float64,1}: the Cobb-Douglas parameters\nc::Array{Float64,1}: the costs of goods\ninitial::Array{Float64,1}: the initial endowment of goods\nbounds::Array{Float64,2}: the bounds on the choice variables\nI::Float64: the budget we can spend\n\nReturns\n\nMySimpleCobbDouglasChoiceProblem: the built Cobb-Douglas choice problem model\n\n\n\n\n\nbuild(modeltype::Type{MyEpsilonGreedyAlgorithmModel}, data::NamedTuple) -> MyEpsilonGreedyAlgorithmModel\n\nBuilds a MyEpsilonGreedyAlgorithmModel from data in a NamedTuple.\n\nArguments\n\nmodeltype::Type{MyEpsilonGreedyAlgorithmModel}: the model type to build\ndata::NamedTuple: the data to use to build the model\n\nThe data NamedTuple must contain the following keys:\n\nK::Int64: number of arms\n\n\n\n\n\nbuild(modeltype::Type{MyExploreFirstAlgorithmModel}, data::NamedTuple) -> MyExploreFirstAlgorithmModel\n\nBuilds a MyExploreFirstAlgorithmModel from data in a NamedTuple.\n\nArguments\n\nmodeltype::Type{MyExploreFirstAlgorithmModel}: the model type to build\ndata::NamedTuple: the data to use to build the model\n\nThe data NamedTuple must contain the following keys:\n\nK::Int64: number of arms\n\n\n\n\n\nbuild(modeltype::Type{MyUCB1AlgorithmModel}, data::NamedTuple) -> MyUCB1AlgorithmModel\n\nBuilds a MyUCB1AlgorithmModel from data in a NamedTuple.\n\nArguments\n\nmodeltype::Type{MyUCB1AlgorithmModel}: the model type to\ndata::NamedTuple: the data to use to build the model\n\nThe data NamedTuple must contain the following keys:\n\nK::Int64: number of arms\n\n\n\n\n\nbuild(modeltype::Type{MyBinaryWeightedMajorityAlgorithmModel}, \n    data::NamedTuple) -> MyBinaryWeightedMajorityAlgorithmModel\n\nBuild a Binary Weighted Majority Algorithm model. This function initializes the model with the given parameters in the data NamedTuple. The model is returned to the caller.\n\nArguments\n\nmodeltype::Type{MyBinaryWeightedMajorityAlgorithmModel}: the type of the model to build\ndata::NamedTuple: the parameters to initialize the model\n\nThe named tuple data must have the following fields:\n\nÏµ::Float64: learning rate\nn::Int64: number of experts\nT::Int64: number of rounds\nexpert::Function: expert function\nadversary::Function: adversary function\n\n\n\n\n\nfunction build(modeltype::Type{MyTwoPersonZeroSumGameModel}, \n    data::NamedTuple)::MyTwoPersonZeroSumGameModel\n\nThis method builds and returns an instance of the MyTwoPersonZeroSumGameModel type.\n\nArguments\n\nmodeltype::Type{MyTwoPersonZeroSumGameModel}: The model type to build.\ndata::NamedTuple: A named tuple containing the model parameters:\nÏµ::Float64: The learning rate.\nn::Int: The number of experts (actions).\nT::Int: The number of rounds.\npayoffmatrix::Array{Float64,2}: The payoff matrix.\n\nReturns\n\nmodel::MyTwoPersonZeroSumGameModel: An instance of the MyTwoPersonZeroSumGameModel type with the specified parameters.\n\n\n\n\n\nbuild(modeltype::Type{MyBinaryVectorArmsEpsilonGreedyAlgorithmModel}, data::NamedTuple) -> MyBinaryVectorArmsEpsilonGreedyAlgorithmModel\n\nBuilds a MyBinaryVectorArmsEpsilonGreedyAlgorithmModel from data in a NamedTuple.\n\nArguments\n\nmodeltype::Type{MyBinaryVectorArmsEpsilonGreedyAlgorithmModel}: the model type to build\ndata::NamedTuple: the data to use to build the model\n\nThe data NamedTuple must contain the following keys:\n\nK::Int64: number of arms\n\nReturns\n\nMyBinaryVectorArmsEpsilonGreedyAlgorithmModel: a populated binary vector arms epsilon greedy algorithm model\n\n\n\n\n\nbuild(modeltype::Type{MyConsumerChoiceBanditContextModel}, data::NamedTuple) -> MyConsumerChoiceBanditContextModel\n\nBuilds a MyConsumerChoiceBanditContextModel from data in a NamedTuple.\n\nArguments\n\nmodeltype::Type{MyConsumerChoiceBanditContextModel}: the model type to build\ndata::NamedTuple: the data to use to build the model\n\nThe data NamedTuple must contain the following keys:\n\ndata::Dict{String, Any}: data dictionary for each item, or more generally the context\nitems::Array{String,1}: items for each asset\nbounds::Array{Float64,2}: bounds on the assets that we can purchase\nB::Float64: budget that we have to spend on the collection of assets\nnâ‚’::Array{Float64,1}: initial guess for the solution\nÎ¼â‚’::Array{Float64,1}: initial for the utility of each arm\nÎ³::Array{Float64,1}: parameters for the utility function (preferences)\n\nReturns\n\nMyConsumerChoiceBanditContextModel: a populated consumer choice bandit context model\n\n\n\n\n\nfunction build(type::Type{MyQLearningModel},data::NamedTuple) -> MyQLearningModel\n\nBuilds a MyQLearningAgentModel from data in a NamedTuple.\n\nArguments\n\nmodeltype::Type{MyQLearningAgentModel}: the model type to build\ndata::NamedTuple: the data to use to build the model\n\nThe data NamedTuple must contain the following keys:\n\nstates::Array{Int64,1}: the state space\nactions::Array{Int64,1}: the action space\nÎ±::Float64: the learning rate\nÎ³::Float64: the discount factor\n\nReturns\n\nMyQLearningAgentModel: a populated Q-learning agent model\n\n\n\n\n\nbuild(modeltype::Type{MyClassicalHopfieldNetworkModel}, data::NamedTuple) -> MyClassicalHopfieldNetworkModel\n\nFactory method for building a Hopfield network model. \n\nArguments\n\nmodeltype::Type{MyClassicalHopfieldNetworkModel}: the type of the model to be built.\ndata::NamedTuple: a named tuple containing the data for the model.\n\nThe named tuple should contain the following fields:\n\nmemories: a matrix of memories (each column is a memory).\n\nReturns\n\nmodel::MyClassicalHopfieldNetworkModel: the built Hopfield network model with the following fields populated:\nW: the weight matrix.\nb: the bias vector.\nenergy: a dictionary of energies for each memory.\n\n\n\n\n\nbuild(modeltype::Type{MyModernHopfieldNetworkModel}, data::NamedTuple) -> MyModernHopfieldNetworkModel\n\nFactory method for assembling a modern Hopfield network model from raw memories and an inverse-temperature parameter.\n\nArguments\n\nmodeltype::Type{MyModernHopfieldNetworkModel}: concrete model type to instantiate.\ndata::NamedTuple: expects memories (matrix with memories on columns) and Î² (inverse-temperature scalar).\n\nReturns\n\nmodel::MyModernHopfieldNetworkModel: model populated with X (memory matrix) and Î².\n\n\n\n\n\nbuild(modeltype::Type{MySimpleBoltzmannMachineModel}, data::NamedTuple) -> MySimpleBoltzmannMachineModel\n\nFactory method for building a simple Boltzmann machine model.\n\nArguments\n\nmodeltype::Type{MySimpleBoltzmannMachineModel}: concrete model type to instantiate.\ndata::NamedTuple: expects W::Array{Float64, 2} (weight matrix) and b::Vector{Float64} (bias vector).\n\nReturns\n\nmodel::MySimpleBoltzmannMachineModel: model populated with W and b.\n\n\n\n\n\nbuild(modeltype::Type{MyRestrictedBoltzmannMachineModel}, data::NamedTuple) -> MyRestrictedBoltzmannMachineModel\n\nFactory method for building a restricted Boltzmann machine model.\n\nArguments\n\nmodeltype::Type{MyRestrictedBoltzmannMachineModel}: concrete model type to instantiate.\ndata::NamedTuple: the data to use to build the model. The NamedTuple must have the following fields:\nW::Array{Float64,2}: weight matrix between visible and hidden units.\nb::Vector{Float64}: bias vector for visible units.\na::Vector{Float64}: bias vector for hidden units.\n\nReturns\n\nmodel::MyRestrictedBoltzmannMachineModel: model populated with W, b, and a.\n\n\n\n\n\nbuild(modeltype::Type{MyNaiveKMeansClusteringAlgorithm}, data::NamedTuple) -> MyNaiveKMeansClusteringAlgorithm\n\nFactory method for building a naive K-Means clustering algorithm model.\n\nArguments\n\nmodeltype::Type{MyNaiveKMeansClusteringAlgorithm}: concrete model type to instantiate.\ndata::NamedTuple: expects the following fields:\nK::Int64: number of clusters\nÏµ::Float64: convergence threshold\nmaxiter::Int64: maximum number of iterations\ndimension::Int64: dimensionality of the data points\nnumber_of_points::Int64: number of data points\nscale_factor::Float64: scale factor for initializing centroids\n\nReturns\n\nmodel::MyNaiveKMeansClusteringAlgorithm: model populated with initial assignments and centroids.\n\n\n\n\n\n","category":"function"},{"location":"factory/#VLDataScienceMachineLearningPackage.populate!","page":"Factory","title":"VLDataScienceMachineLearningPackage.populate!","text":"populate!(model::MyAdjacencyRecombiningCommodityPriceTree, price::Float64, Î”::Array{Float64,1})\n\nThis function populates the price tree model with the given price and price change factors. This method updates the model in place.\n\nArguments\n\nmodel::MyAdjacencyRecombiningCommodityPriceTree: The price tree model to populate.\nprice::Float64: The initial price to set at the root of the tree.\nÎ”::Array{Float64,1}: The array of price change factors for each level of the tree.\n\n\n\n\n\nfunction populate!(model::MyFullGeneralAdjacencyTree, configuration::Function)::MyFullGeneralAdjacencyTree\n\nPopulates the data for the tree model using the provided configuration function. \n\nArguments\n\nmodel::MyFullGeneralAdjacencyTree: The tree model instance to populate.\nconfiguration::Function: A function that takes four arguments (level, index, offset and parentdatapayload) and returns a NamedTuple with the configuration data for that node.\n\nReturns\n\nMyFullGeneralAdjacencyTree: The updated tree model with populated data.\n\n\n\n\n\n","category":"function"},{"location":"clustering/#Clustering-and-Classification-Algorithms","page":"Clustering Algorithms","title":"Clustering and Classification Algorithms","text":"This section provides an overview of various clustering algorithms implemented in the VLDataScienceMachineLearningPackage.jl. Clustering is an unsupervised learning technique used to group similar data points together based on their features.","category":"section"},{"location":"clustering/#K-Means-Clustering","page":"Clustering Algorithms","title":"K-Means Clustering","text":"K-Means is a popular clustering algorithm that partitions data into K distinct clusters based on feature similarity. The algorithm iteratively assigns data points to the nearest cluster centroid and updates the centroids until convergence.","category":"section"},{"location":"clustering/#VLDataScienceMachineLearningPackage.MyNaiveKMeansClusteringAlgorithm","page":"Clustering Algorithms","title":"VLDataScienceMachineLearningPackage.MyNaiveKMeansClusteringAlgorithm","text":"mutable struct MyNaiveKMeansClusteringAlgorithm <: MyAbstractUnsupervisedClusteringAlgorithm\n\nA mutable struct that represents a naive K-Means clustering algorithm.\n\nFields\n\nK::Int64: number of clusters\ncentroids::Dict{Int64, Vector{Float64}}: cluster centroids\nassignments::Vector{Int64}: cluster assignments\nÏµ::Float64: convergence criteria\nmaxiter::Int64: maximum number of iterations\ndimension::Int64: dimension of the data\nnumber_of_points::Int64: number of data points\n\n\n\n\n\n","category":"type"},{"location":"clustering/#VLDataScienceMachineLearningPackage.cluster","page":"Clustering Algorithms","title":"VLDataScienceMachineLearningPackage.cluster","text":"cluster(data::Array{<:Number,2}, algorithm::T; d = Euclidean(), verbose::Bool = false) where T <: MyAbstractUnsupervisedClusteringAlgorithm\n\nCluster the input data using the specified clustering algorithm.\n\nArguments\n\ndata::Array{<:Number,2}: A 2D array where each row represents a data point.\nalgorithm::T: An instance of a clustering algorithm type T that is a subtype of MyAbstractUnsupervisedClusteringAlgorithm.\nd: (Optional) A distance metric function. Default is Euclidean().\nverbose::Bool: (Optional) A boolean flag to enable verbose output. Default is false.\ntmpdir::String: (Optional) A directory path for temporary files. Default is the current directory \".\".\n\nReturns\n\nA tuple containing the clustering assignments, centroids, and the number of iterations taken to converge.\n\n\n\n\n\n","category":"function"},{"location":"bandit/#Bandit-Algorithms","page":"Bandit Algorithms","title":"Bandit Algorithms","text":"This section covers the implementation of several multi-armed bandit algorithms in the VLDataScienceMachineLearningPackage.jl package. ","category":"section"},{"location":"bandit/#Binary-arms","page":"Bandit Algorithms","title":"Binary arms","text":"Suppose we wanted to look at a bandit problem where each arm is represented by a binary vector of length K. This means that there are a total of 2^K possible arms, each corresponding to a unique combination of the K binary features. Further, the reward structure is computed in a user specified world function that maps each binary vector to a reward, along with a context model.\n\nWe've built in support for this type of bandit problem in the package. Specifically, we have defined a new abstract type AbstractBanditAlgorithmContextModel that can be used to represent the context of a bandit problem with binary arms. We have a concrete implementation of this type called MyConsumerChoiceBanditContextModel, which allows users to specify the average rewards for each possible combination of binary features.","category":"section"},{"location":"bandit/#VLDataScienceMachineLearningPackage.MyExploreFirstAlgorithmModel","page":"Bandit Algorithms","title":"VLDataScienceMachineLearningPackage.MyExploreFirstAlgorithmModel","text":"mutable struct MyExploreFirstAlgorithmModel <: AbstractBanditAlgorithmModel\n\nA mutable struct that represents an explore-first bandit algorithm model.\n\n\n\n\n\n","category":"type"},{"location":"bandit/#VLDataScienceMachineLearningPackage.MyEpsilonGreedyAlgorithmModel","page":"Bandit Algorithms","title":"VLDataScienceMachineLearningPackage.MyEpsilonGreedyAlgorithmModel","text":"mutable struct MyEpsilonGreedyAlgorithmModel <: AbstractBanditAlgorithmModel\n\nA mutable struct that represents an epsilon-greedy bandit algorithm model.\n\n\n\n\n\n","category":"type"},{"location":"bandit/#VLDataScienceMachineLearningPackage.MyUCB1AlgorithmModel","page":"Bandit Algorithms","title":"VLDataScienceMachineLearningPackage.MyUCB1AlgorithmModel","text":"mutable struct MyUCB1AlgorithmModel <: AbstractBanditAlgorithmModel\n\nA mutable struct that represents a UCB1 bandit algorithm model.\n\n\n\n\n\n","category":"type"},{"location":"bandit/#VLDataScienceMachineLearningPackage.solve-Tuple{AbstractBanditAlgorithmModel}","page":"Bandit Algorithms","title":"VLDataScienceMachineLearningPackage.solve","text":"solve(model::AbstractBanditAlgorithmModel; T::Int = 0, world::Function = _null)\n\nSolve the bandit problem using the given model. \n\nArguments\n\nmodel::AbstractBanditAlgorithmModel: The model to use to solve the bandit problem.\nT::Int = 0: The number of rounds to play. Default is 0.\nworld::Function = _null: The function that returns the reward for a given action. Default is the private _null function.\n\nReturns\n\nArray{Float64,2}: The rewards for each arm at each round.\n\n\n\n\n\n","category":"method"},{"location":"bandit/#VLDataScienceMachineLearningPackage.regret","page":"Bandit Algorithms","title":"VLDataScienceMachineLearningPackage.regret","text":"regret(rewards::Array{Float64,2})::Array{Float64,1}\n\nCompute the regret for the given rewards.\n\nArguments\n\nrewards::Array{Float64,2}: The rewards for each arm at each round.\n\nReturns\n\nArray{Float64,1}: The regret at each round.\n\n\n\n\n\n","category":"function"},{"location":"bandit/#VLDataScienceMachineLearningPackage.MyBinaryVectorArmsEpsilonGreedyAlgorithmModel","page":"Bandit Algorithms","title":"VLDataScienceMachineLearningPackage.MyBinaryVectorArmsEpsilonGreedyAlgorithmModel","text":"mutable struct MyBinaryVectorArmsEpsilonGreedyAlgorithmModel <: AbstractBanditAlgorithmModel\n\nA mutable struct that represents a binary vector arms epsilon-greedy bandit algorithm model.\n\nFields\n\nK::Int64: number of arms\n\n\n\n\n\n","category":"type"},{"location":"bandit/#VLDataScienceMachineLearningPackage.MyConsumerChoiceBanditContextModel","page":"Bandit Algorithms","title":"VLDataScienceMachineLearningPackage.MyConsumerChoiceBanditContextModel","text":"mutable struct MyConsumerChoiceBanditContextModel <: AbstractBanditProblemContextModel\n\nThis struct defines a consumer choice bandit context model for bandit problems with context.\n\nFields\n\ndata::Dict{String, Any}: A dictionary containing the context data for each item.\nitems::Array{String,1}: An array of item names (things we are purchasing).\nbounds::Array{Float64,2}: A 2D array defining the bounds on the items that we can purchase.\nB::Float64: The budget available to spend on the collection of items.\nnâ‚’::Array{Float64,1}: An initial guess for the solution (quantities of each item).\nÎ¼â‚’::Array{Float64,1}: An initial estimate for the utility of each arm (collection of items).\n\n\n\n\n\n","category":"type"},{"location":"bandit/#VLDataScienceMachineLearningPackage.solve-Tuple{MyBinaryVectorArmsEpsilonGreedyAlgorithmModel, MyConsumerChoiceBanditContextModel}","page":"Bandit Algorithms","title":"VLDataScienceMachineLearningPackage.solve","text":"solve(model::AbstractBanditAlgorithmModel, context::AbstractBanditProblemContextModel;\n    T::Int = 0, world::Function = _null)\n\nSolve the contextual bandit problem using the given model and context.\n\nArguments\n\nmodel::MyBinaryVectorArmsEpsilonGreedyAlgorithmModel: The model to use to solve the bandit problem.\ncontext::MyConsumerChoiceBanditContextModel: The context model to use. Must be a subtype of AbstractBanditProblemContextModel.\nT::Int = 0: The number of rounds to play. Default is 0.\nworld::Function = _null: The function that returns the reward for a given action. Default is the private _null function.\nstartdayindex::Int = 1: The starting time index for the context model. Default is 1. This is useful for time series data.\n\n\n\n\n\n","category":"method"},{"location":"wolfram/#Cellular-Automata","page":"Wolfram","title":"Cellular Automata","text":"Cellular automata are discrete models studied in computational theory, mathematics, and theoretical biology. They consist of a grid of cells, each in one of a finite number of states, and evolve through simple rules based on the states of neighboring cells.\n\nThis is a fascinating area of study with applications in various fields, including physics, biology, and computer science. Here, we explore the Wolfram model of cellular automata, which provides a framework for understanding complex systems and emergent behavior.","category":"section"},{"location":"wolfram/#VLDataScienceMachineLearningPackage.simulate","page":"Wolfram","title":"VLDataScienceMachineLearningPackage.simulate","text":"function simulate(rulemodel::MyOneDimensionalElementaryWolframRuleModel, initial::Array{Int64,1};\n    steps::Int64 = 24, maxnumberofmoves::Union{Int64, Nothing} = nothing, \n    algorithm::AbstractWolframSimulationAlgorithm)) -> Dict{Int64, Array{Int64,2}}\n\nThe simulate function runs a Wolfram simulation based on the provided rule model and initial state.\n\nArguments\n\nrulemodel::MyOneDimensionalElementaryWolframRuleModel: The rule model to use for the simulation.\ninitial::Array{Int64,1}: The initial state of the simulation.\nsteps::Int64: The number of steps to simulate.\nmaxnumberofmoves::Union{Int64, Nothing}: The maximum number of moves to simulate.\nalgorithm::AbstractWolframSimulationAlgorithm: The algorithm to use for the simulation.\n\nReturns\n\nA dictionary mapping step numbers to the state of the simulation at that step.\n\n\n\n\n\n","category":"function"},{"location":"#VLDataScienceMachineLearningPackage.jl","page":"Home","title":"VLDataScienceMachineLearningPackage.jl","text":"The VLDataScienceMachineLearningPackage.jl package is a Julia package that provides functions and types useful for data science, machine learning, and artificial intelligence applications. The package is designed to be simple and easy to use, and it is suitable for students, researchers, and practitioners in the data science and machine learning areas.","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"The package can be installed by running the following command in the Julia REPL:\n\nusing Pkg\nPkg.add(url=\"https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl.git\")","category":"section"},{"location":"types/#Types","page":"Types","title":"Types","text":"We'll work with may types in this package, including some abstract types that will be used to define the structure of our models.","category":"section"},{"location":"types/#VLDataScienceMachineLearningPackage.MySarcasmRecordModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MySarcasmRecordModel","text":"MySarcasmRecordModel <: AbstractTextRecordModel\n\nModel for a record in the Sarcasm dataset.\n\nFields\n\ndata::Array{String, Any} - The data found in the record in the order they were found\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MySarcasmRecordCorpusModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MySarcasmRecordCorpusModel","text":"MySarcasmRecordCorpusModel <: AbstractTextDocumentCorpusModel\n\nModel for a collection of records in the Sarcasm dataset.\n\nFields\n\nrecords::Dict{Int, MySarcasmRecordModel} - The records in the document (collection of records)\ntokens::Dict{String, Int64} - A dictionary of tokens in alphabetical order (key: token, value: position) for the entire document\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MySMSSpamHamRecordModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MySMSSpamHamRecordModel","text":"MySMSSpamHamRecordModel <: AbstractTextRecordModel\n\nModel for a record in the SMS Spam Ham dataset.\n\nFields\n\nisspam::Bool - a boolean value indicating if the message is spam.\nmessage::String - the content of the SMS message.\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MySMSSpamHamRecordCorpusModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MySMSSpamHamRecordCorpusModel","text":"MySMSSpamHamRecordCorpusModel <: AbstractTextDocumentCorpusModel\n\nModel for a collection of records in the SMS Spam Ham dataset.\n\nFields\n\nrecords::Dict{Int, MySMSSpamHamRecordModel} - The records in the document (collection of records)\ntokens::Dict{String, Int64} - A dictionary of tokens in alphabetical order (key: token, value: position) for the entire document\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MyFullGeneralAdjacencyTree","page":"Types","title":"VLDataScienceMachineLearningPackage.MyFullGeneralAdjacencyTree","text":"mutable struct MyFullGeneralAdjacencyTree <: AbstractTreeModel\n\nThe MyFullGeneralAdjacencyTree type is a model of a full general adjacency tree that uses a dictionary to store the tree structure. There is a build and populate! method to build the tree and populate it with data.\n\nFields\n\ndata::Union{Nothing, Dict{Int64,NamedTuple}} - A dictionary that stores the node data for the tree.\nconnectivity::Dict{Int64,Array{Int64,1}} - A dictionary that stores the connectivity information between nodes.\nh::Int64 - The height of the tree.\nn::Int64 - The branching factor of the tree.\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MyAdjacencyRecombiningCommodityPriceTree","page":"Types","title":"VLDataScienceMachineLearningPackage.MyAdjacencyRecombiningCommodityPriceTree","text":"mutable struct  MyAdjacencyRecombiningCommodityPriceTree <: AbstractPriceTreeModel\n\nThe MyAdjacencyRecombiningCommodityPriceTree type is a model of a commodity price tree that uses an dictionary to store the price data. This model stores the connectivity information between nodes.\n\nFields\n\ndata::Union{Nothing, Dict{Int64,NamedTuple}} - A dictionary that stores the price data and path informationfor the tree.\nconnectivity::Dict{Int64,Array{Int64,1}} - A dictionary that stores the connectivity information between nodes.\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MyOneDimensionalElementaryWolframRuleModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MyOneDimensionalElementaryWolframRuleModel","text":"mutable struct MyOneDimensionalElementaryWolframRuleModel <: AbstractRuleModel\n\nThe MyOneDimensionalElementarWolframRuleModel mutable struct represents a one-dimensional elementary Wolfram rule model.\n\nFields\n\nindex::Int - The index of the rule\nradius::Int - The radius, i.e, the number of cells that influence the next state for this rule\nrule::Dict{Int,Int} - A dictionary that holds the rule where the key is the binary representation of the neighborhood and the value is the next state\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MyGraphNodeModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MyGraphNodeModel","text":"mutable struct MyGraphNodeModel\n\nA lightweight mutable node model used in simple graph representations.\n\nFields\n\nid::Int64 - Unique integer identifier for the node.\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MyGraphEdgeModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MyGraphEdgeModel","text":"mutable struct MyGraphEdgeModel\n\nA mutable edge model representing a directed or undirected connection between nodes. The model stores a numeric id, endpoint indices, and an optional numeric weight.\n\nFields\n\nid::Int64 - Unique integer identifier for the edge.\nsource::Int64 - Identifier of the source node (or one endpoint).\ntarget::Int64 - Identifier of the target node (or the other endpoint).\nweight::Union{Nothing, Any} - Optional edge weight; nothing indicates an unweighted edge.\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MySimpleDirectedGraphModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MySimpleDirectedGraphModel","text":"mutable struct MySimpleDirectedGraphModel\n\nA minimal mutable directed graph container that keeps node and edge registries and a children adjacency map for fast traversal of outgoing neighbors.\n\nFields\n\nnodes::Union{Nothing, Dict{Int64, MyGraphNodeModel}} - Optional mapping from node id to MyGraphNodeModel. Use nothing when uninitialized.\nedges::Union{Nothing, Dict{Tuple{Int, Int}, Int64}} - Optional mapping from (source, target) tuple to edge id. Use nothing when uninitialized.\nchildren::Union{Nothing, Dict{Int64, Set{Int64}}} - Optional adjacency map from a node id to the set of its child (outgoing) node ids.\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MySimpleUndirectedGraphModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MySimpleUndirectedGraphModel","text":"mutable struct MySimpleUndirectedGraphModel\n\nA minimal mutable undirected graph container that keeps node and edge registries and a children adjacency map for fast traversal of outgoing neighbors.\n\nFields\n\nnodes::Union{Nothing, Dict{Int64, MyGraphNodeModel}} - Optional mapping from node id to MyGraphNodeModel. Use nothing when uninitialized.\nedges::Union{Nothing, Dict{Tuple{Int, Int}, Int64}} - Optional mapping from (source, target) tuple to edge id. Use nothing when uninitialized.\nchildren::Union{Nothing, Dict{Int64, Set{Int64}}} - Optional adjacency map from a node id to the set of its child (outgoing) node ids.\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MyDirectedBipartiteGraphModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MyDirectedBipartiteGraphModel","text":"mutable struct MyDirectedBipartiteGraphModel <: AbstractGraphModel\n\nThis type models a directed bipartite graph with source and sink nodes, along with maximum capacity constraints on the edges. This type is constructed using a build method.\n\nFields\n\nnodes::Union{Nothing, Dict{Int64, MyGraphNodeModel}} - Optional mapping from node id to MyGraphNodeModel. Use nothing when uninitialized.\nedges::Union{Nothing, Dict{Tuple{Int, Int}, Int64}} - Optional mapping from (source, target) tuple to edge id. Use nothing when uninitialized.\nchildren::Union{Nothing, Dict{Int64, Set{Int64}}} - Optional adjacency map from a node id to the set of its child (outgoing) node ids.\nedgesinverse::Dict{Int, Tuple{Int, Int}} - Map between edge id and (source, target) tuple.\nleft::Set{Int64} - Set of left (source) node ids.\nright::Set{Int64} - Set of right (sink) node ids.\nsource::Int64 - Source node id.\nsink::Int64 - Sink node id.\ncapacity::Dict{Tuple{Int64, Int64}, Tuple{Number, Number}} - Capacity constraints on the edges.\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MyConstrainedGraphEdgeModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MyConstrainedGraphEdgeModel","text":"mutable struct MyConstrainedGraphEdgeModel\n\nMutable model for a graph edge with capacity constraints. \n\nFields\n\nid::Int64 - Unique identifier for the edge.\nsource::Int64 - Identifier for the source node.\ntarget::Int64 - Identifier for the target node.\nlower::Union{Nothing, Number} - Lower capacity constraint for the edge.\nupper::Union{Nothing, Number} - Upper capacity constraint for the edge.\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MyPerceptronClassificationModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MyPerceptronClassificationModel","text":"mutable struct MyPerceptronClassificationModel <: AbstractClassificationAlgorithm\n\nA mutable struct that represents a perceptron classification model.\n\nFields\n\n- `Î²::Vector{Float64}`: coefficients\n- `mistakes::Int64`: number of mistakes that are are willing to make\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MyLogisticRegressionClassificationModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MyLogisticRegressionClassificationModel","text":"mutable struct MyLogisticRegressionClassificationModel <: AbstractClassificationAlgorithm\n\nA mutable struct that represents a logistic regression classification model.\n\nFields\n\n- `Î²::Vector{Float64}`: coefficients\n- `Î±::Float64`: learning rate\n- `Ïµ::Float64`: convergence criterion\n- `L::Function`: loss function\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MyLinearProgrammingProblemModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MyLinearProgrammingProblemModel","text":"mutable struct MyLinearProgrammingProblemModel <: AbstractLinearProgrammingProblemType\n\nA mutable struct that represents a linear programming problem model.\n\nFields\n\nA::Array{Float64,2}: constraint matrix\nb::Array{Float64,1}: right-hand side vector\nc::Union{Array{Float64,2}, Array{Float64,1}}: objective function coefficient matrix (vector)\nlb::Array{Float64,1}: lower bound vector\nub::Array{Float64,1}: upper bound vector  \n\n\n\n\n\n","category":"type"}]
}
