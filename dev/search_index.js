var documenterSearchIndex = {"docs":
[{"location":"graphs/#Graphs","page":"Graphs","title":"Graphs","text":"","category":"section"},{"location":"graphs/","page":"Graphs","title":"Graphs","text":"We work with various graph structures in this package, including directed and undirected graphs, as well as specialized graph types for specific applications.","category":"page"},{"location":"graphs/#VLDataScienceMachineLearningPackage.children","page":"Graphs","title":"VLDataScienceMachineLearningPackage.children","text":"function children(graph::T, node::MyGraphNodeModel) -> Set{Int64} where T <: AbstractGraphModel\n\nReturns the set of child node IDs for a given node in the graph.\n\nArguments\n\ngraph::T: The graph to search where T <: AbstractGraphModel.\nnode::MyGraphNodeModel: The node to find children for.\n\nReturns\n\nSet{Int64}: The set of child node IDs.\n\n\n\n\n\n","category":"function"},{"location":"graphs/#VLDataScienceMachineLearningPackage.weight","page":"Graphs","title":"VLDataScienceMachineLearningPackage.weight","text":"function weight(graph::T, source::Int64, target::Int64, edgemodels::Dict{Int64, MyGraphEdgeModel}) -> Any where T <: AbstractGraphModel\n\nReturns the weight of the edge between two nodes in the graph.\n\nArguments\n\ngraph::T: The graph to search where T <: AbstractGraphModel.\nsource::Int64: The ID of the source node.\ntarget::Int64: The ID of the target node.\n\nReturns\n\nAny: The weight of the edge between the source and target nodes. We have this as Any to allow for flexibility in edge weights, which can be of any type.\n\n\n\n\n\nfunction weight(graph::T, source::Int64, target::Int64) -> Float64 where T <: AbstractGraphModel\n\nThis function returns the weight of the edge between two nodes in a graph model.\n\nArguments\n\ngraph::T: the graph model to search. This is a subtype of AbstractGraphModel.\nsource::Int64: the source node id.\ntarget::Int64: the target node id.\n\nReturns\n\nthe weight of the edge between the source and target nodes.\n\n\n\n\n\n","category":"function"},{"location":"graphs/#VLDataScienceMachineLearningPackage.walk","page":"Graphs","title":"VLDataScienceMachineLearningPackage.walk","text":"function walk(graph::T, startnode::MyGraphNodeModel, algorithm::AbstractGraphTraversalAlgorithm; \nverbose::Bool = false) where T <: AbstractGraphModel\n\nThe walk function traverses the graph starting from a given node using the specified algorithm (either Depth-First Search or Breadth-First Search).  It maintains a set of visited nodes to avoid cycles and ensure that each node is processed only once.\n\nArguments\n\ngraph::T: The graph to traverse.\nstartnode::MyGraphNodeModel: The node to start the traversal from.\nalgorithm::AbstractGraphTraversalAlgorithm: The algorithm to use for the traversal. This can be either an instance of DepthFirstSearchAlgorithm or BreadthFirstSearchAlgorithm. Default is BreadthFirstSearchAlgorithm.\nverbose::Bool: Whether to print verbose output (default is false).\n\nReturns\n\nArray{Int64,1}: The collection of visited node IDs in the order they were visited.\n\n\n\n\n\n","category":"function"},{"location":"graphs/#VLDataScienceMachineLearningPackage.findshortestpath","page":"Graphs","title":"VLDataScienceMachineLearningPackage.findshortestpath","text":"findshortestpath(graph::T, start::MyGraphNodeModel; \n    algorithm::AbstractGraphSearchAlgorithm = BellmanFordAlgorithm()) where T <: AbstractGraphModel\n\nThe function computes the shortest paths from a starting node to all other nodes in a graph model. \n\nArguments\n\ngraph::T: the graph model to search. This is a subtype of AbstractGraphModel.\nstart::MyGraphNodeModel: the node to start the search from.\nalgorithm::MyAbstractGraphSearchAlgorithm: the algorithm to use for the search. The default is BellmanFordAlgorithm, but it can also be DijkstraAlgorithm.\n\nReturns\n\na tuple of two dictionaries: the first dictionary contains the distances from the starting node to all other nodes, and the second dictionary contains the previous node in the shortest path from the starting node to all other nodes.\n\n\n\n\n\n","category":"function"},{"location":"graphs/#VLDataScienceMachineLearningPackage.maximumflow","page":"Graphs","title":"VLDataScienceMachineLearningPackage.maximumflow","text":"function maximumflow(graph::T, source::MyGraphNodeModel, sink::MyGraphNodeModel;\n    algorithm::AbstractGraphFlowAlgorithm = FordFulkersonAlgorithm()) where T <: AbstractGraphModel\n\nThis function computes the maximum flow in a directed graph from a source node to a sink node using the specified algorithm. The current implementation does not use the lower bound on the edge capacity (assumes all edges have a lower capacity bound of 0).\n\nArguments\n\ngraph::T: The graph to search. This needs to be a directed graph, with capacities on the edges. \nsource::MyGraphNodeModel: The source node.\nsink::MyGraphNodeModel: The sink node.\nalgorithm::AbstractGraphFlowAlgorithm: The algorithm to use for the search. The default is FordFulkersonAlgorithm. The EdmondsKarpAlgorithm can also be used.\natol::Float64: The absolute tolerance for floating point comparisons (default is 1e-8).\n\nReturns\n\nFloat64: The maximum flow value.\nDict{{Int64,Int64}, Number}: A dictionary mapping each edge (as a tuple of source and target node IDs) to its flow value.\n\n\n\n\n\n","category":"function"},{"location":"text/#Working-with-Text-Data","page":"Text","title":"Working with Text Data","text":"","category":"section"},{"location":"text/","page":"Text","title":"Text","text":"We'll work with text data in many applications. We've included a few methods to help with text processing. ","category":"page"},{"location":"text/#VLDataScienceMachineLearningPackage.tokenize","page":"Text","title":"VLDataScienceMachineLearningPackage.tokenize","text":"function tokenize(s::String, tokens::Dict{String, Int64}; \n    pad::Int64 = 0, padleft::Bool = false, delim::Char = ' ') -> Array{Int64,1}\n\nArguments\n\ns::String - the string to tokenize.\ntokens::Dict{String, Int64} - a dictionary of tokens in alphabetical order (key: token, value: position) for the entire document.\npad::Int64 - (optional) the number of padding tokens to add to the end of the tokenized string. Default is 0.\npadleft::Bool - (optional) if true, the padding tokens are added to the beginning of the tokenized string. Default is false.\ndelim::Char - (optional) the delimiter used in the string. Default is ' '.\n\nReturns\n\nArray{Int64,1} - an array of integers representing the vectorized string.\n\n\n\n\n\n","category":"function"},{"location":"text/#VLDataScienceMachineLearningPackage.featurehashing","page":"Text","title":"VLDataScienceMachineLearningPackage.featurehashing","text":"function featurehashing(text::Array{String,1}; d::Int64 = 100, \n    algorithm::AbstractFeatureHashingAlgorithm = UnsignedFeatureHasing()) -> Array{Int64,1}\n\nComputes the feature hashing of the input text using the specified algorithm.\n\nArguments\n\ntext::Array{String,1} - an array of strings to be hashed.\nd::Int64 - (optional) the size of the hash table. Default is 100.\nalgorithm::AbstractFeatureHasingAlgorithm - (optional) the hashing algorithm to use. Default is UnsignedFeatureHasing.\n\nReturns\n\nArray{Int64,1} - an array of integers representing the hashed features.\n\n\n\n\n\nfunction featurehashing(text::Array{Int,1}; d::Int64 = 100, \n    algorithm::AbstractFeatureHashingAlgorithm = UnsignedFeatureHasing()) -> Array{Int64,1}\n\nComputes the feature hashing of the input text using the specified algorithm.\n\nArguments\n\ntext::Array{Int,1} - an array of integers to be hashed (e.g., tokenized text).\nd::Int64 - (optional) the size of the hash table. Default is 100.\nalgorithm::AbstractFeatureHasingAlgorithm - (optional) the hashing algorithm to use. Default is UnsignedFeatureHasing.\n\nReturns\n\nArray{Int64,1} - an array of integers representing the hashed features.\n\n\n\n\n\n","category":"function"},{"location":"factory/#Factory-methods","page":"Factory","title":"Factory methods","text":"","category":"section"},{"location":"factory/","page":"Factory","title":"Factory","text":"We use a particular pattern to build and configure the various composite types in our system. This pattern involves the use of factory functions that encapsulate the construction logic for each type, ensuring that all necessary dependencies are properly initialized and configured.","category":"page"},{"location":"factory/#VLDataScienceMachineLearningPackage.build","page":"Factory","title":"VLDataScienceMachineLearningPackage.build","text":"function build(modeltype::Type{T}, \n    data::NamedTuple)::T where T <: AbstractLinearProgrammingProblemType\n\nThe function builds a linear programming problem model from the data provided.\n\nArguments\n\nmodeltype::Type{T}: the type of the model to build where T is a subtype of AbstractLinearProgrammingProblemType.\ndata::NamedTuple: the data to use to build the model.   \n\nThe data::NamedTuple must have the following fields:\n\nA::Array{Float64,2}: the constraint matrix.\nb::Array{Float64,1}: the right-hand side vector.\nc::Array{Float64,1}: the cost vector.\nlb::Array{Float64,1}: the lower bounds vector.\nub::Array{Float64,1}: the upper bounds vector.\n\nReturns\n\na linear programming problem model of type T where T is a subtype of AbstractLinearProgrammingProblemType.\n\n\n\n\n\nbuild(modeltype::Type{MyPerceptronClassificationModel}, \n    data::NamedTuple) -> MyPerceptronClassificationModel\n\nThe function builds a perceptron classification model from the data provided.\n\nArguments\n\nmodeltype::Type{MyPerceptronClassificationModel}: the type of the model to build.\ndata::NamedTuple: the data to use to build the model.\n\nThe data::NamedTuple must have the following fields:\n\nparameters::Vector{Float64}: the coefficients of the model.\nmistakes::Int64: the number of mistakes that are are willing to make.\n\nReturns\n\na perceptron classification model.\n\n\n\n\n\nfunction build(type::Type{MyAdjacencyRecombiningCommodityPriceTree}, data::NamedTuple) -> MyAdjacencyRecombiningCommodityPriceTree\n\nBuilds an MyAdjacencyRecombiningCommodityPriceTree model given the data in the NamedTuple.  This method builds the connectivity of the tree. To compute the price at each node, use the populate! method.\n\nArguments\n\ntype::Type{MyAdjacencyRecombiningCommodityPriceTree}: The type of the model to build.\ndata::NamedTuple: The data to use to build the model.\n\nThe data NamedTuple must contain the following fields:\n\nh::Int64: The height of the tree.\nprice::Float64: The price at the root node.\nu::Float64: The price increase factor.\nd::Float64: The price decrease factor.\n\nReturns\n\nMyAdjacencyRecombiningCommodityPriceTree: the price tree model holding the computed price data.\n\n\n\n\n\nfunction build(modeltype::Type{MyFullGeneralAdjacencyTree}, data::NamedTuple) -> MyFullGeneralAdjacencyTree\n\nThis function builds a MyFullGeneralAdjacencyTree model given the data in the NamedTuple.  It populates the connectivity of the tree. However, it does not populate the data for the tree nodes. We populate the data using the populate! method.\n\nArguments\n\nmodeltype::Type{MyFullGeneralAdjacencyTree}: The type of the model to build.\ndata::NamedTuple: The data to use to build the model. The NamedTuple must have the following fields:\nh::Int64: The height of the tree.\nn::Int64: The branching factor of the tree.\n\nReturns\n\nMyFullGeneralAdjacencyTree: The constructed tree model.\n\n\n\n\n\nfunction build(modeltype::Type{MyOneDimensionalElementaryWolframRuleModel}, data::NamedTuple) -> MyOneDimensionalElementarWolframRuleModel\n\nThis build method constructs an instance of the MyOneDimensionalElementaryWolframRuleModel type using the data in a NamedTuple.\n\nArguments\n\nmodeltype::Type{MyOneDimensionalElementaryWolframRuleModel}: The type of model to build, in this case, the MyOneDimensionalElementaryWolframRuleModel type.\ndata::NamedTuple: The data to use to build the model.\n\nThe data::NamedTuple must contain the following keys:\n\nindex::Int64: The index of the Wolfram rule\ncolors::Int64: The number of colors in the rule\nradius::Int64: The radius, i.e., the number of cells to consider in the rule\n\nReturn\n\nThis function returns a populated instance of the MyOneDimensionalElementaryWolframRuleModel type.\n\n\n\n\n\nfunction build(model::Type{T}, edgemodels::Dict{Int64, MyGraphEdgeModel}) where T <: AbstractGraphModel\n\nThis function builds a graph model from a dictionary of edge models.\n\nArguments\n\nmodel::Type{T}: The type of graph model to build, where T is a subtype of AbstractGraphModel.\nedgemodels::Dict{Int64, MyGraphEdgeModel}: A dictionary of edge models to use for building the graph.\n\nReturns\n\nT: The constructed graph model, where T is a subtype of AbstractGraphModel.\n\n\n\n\n\nfunction build(modeltype::Type{MyDirectedBipartiteGraphModel}, data::NamedTuple) -> MyDirectedBipartiteGraphModel\n\nThis function builds a mutable MyDirectedBipartiteGraphModel instance given the data in the data::NamedTuple argument.\n\nArguments\n\nmodeltype::Type{MyDirectedBipartiteGraphModel} - The type of the model to build.\ndata::NamedTuple - The data to populate the model with.\n\nThe data::NamedTuple argument must contain the following fields:\n\ns::Int64 - The source node index.\nt::Int64 - The target node index.\nedges::Dict{Int, MyConstrainedGraphEdgeModel} - The edges dictionary containing the constrained graph edges models.\n\n\n\n\n\n","category":"function"},{"location":"factory/#VLDataScienceMachineLearningPackage.populate!","page":"Factory","title":"VLDataScienceMachineLearningPackage.populate!","text":"populate!(model::MyAdjacencyRecombiningCommodityPriceTree, price::Float64, Δ::Array{Float64,1})\n\nThis function populates the price tree model with the given price and price change factors. This method updates the model in place.\n\nArguments\n\nmodel::MyAdjacencyRecombiningCommodityPriceTree: The price tree model to populate.\nprice::Float64: The initial price to set at the root of the tree.\nΔ::Array{Float64,1}: The array of price change factors for each level of the tree.\n\n\n\n\n\nfunction populate!(model::MyFullGeneralAdjacencyTree, configuration::Function)::MyFullGeneralAdjacencyTree\n\nPopulates the data for the tree model using the provided configuration function. \n\nArguments\n\nmodel::MyFullGeneralAdjacencyTree: The tree model instance to populate.\nconfiguration::Function: A function that takes four arguments (level, index, offset and parentdatapayload) and returns a NamedTuple with the configuration data for that node.\n\nReturns\n\nMyFullGeneralAdjacencyTree: The updated tree model with populated data.\n\n\n\n\n\n","category":"function"},{"location":"wolfram/#Cellular-Automata","page":"Wolfram","title":"Cellular Automata","text":"","category":"section"},{"location":"wolfram/","page":"Wolfram","title":"Wolfram","text":"Cellular automata are discrete models studied in computational theory, mathematics, and theoretical biology. They consist of a grid of cells, each in one of a finite number of states, and evolve through simple rules based on the states of neighboring cells.","category":"page"},{"location":"wolfram/","page":"Wolfram","title":"Wolfram","text":"This is a fascinating area of study with applications in various fields, including physics, biology, and computer science. Here, we explore the Wolfram model of cellular automata, which provides a framework for understanding complex systems and emergent behavior.","category":"page"},{"location":"wolfram/#VLDataScienceMachineLearningPackage.simulate","page":"Wolfram","title":"VLDataScienceMachineLearningPackage.simulate","text":"function simulate(rulemodel::MyOneDimensionalElementaryWolframRuleModel, initial::Array{Int64,1};\n    steps::Int64 = 24, maxnumberofmoves::Union{Int64, Nothing} = nothing, \n    algorithm::AbstractWolframSimulationAlgorithm)) -> Dict{Int64, Array{Int64,2}}\n\nThe simulate function runs a Wolfram simulation based on the provided rule model and initial state.\n\nArguments\n\nrulemodel::MyOneDimensionalElementaryWolframRuleModel: The rule model to use for the simulation.\ninitial::Array{Int64,1}: The initial state of the simulation.\nsteps::Int64: The number of steps to simulate.\nmaxnumberofmoves::Union{Int64, Nothing}: The maximum number of moves to simulate.\nalgorithm::AbstractWolframSimulationAlgorithm: The algorithm to use for the simulation.\n\nReturns\n\nA dictionary mapping step numbers to the state of the simulation at that step.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Linear-Systems","page":"Linear","title":"Linear Systems","text":"","category":"section"},{"location":"solvers/","page":"Linear","title":"Linear","text":"We provide several techniques for working with linear systems, including eigendecomposition using QR iteration and iterative solvers for linear systems. These solvers are useful for large, sparse systems where direct methods may be inefficient.","category":"page"},{"location":"solvers/","page":"Linear","title":"Linear","text":"In addition, we provide functionality for solving linear programming problems.","category":"page"},{"location":"solvers/#VLDataScienceMachineLearningPackage.qriteration","page":"Linear","title":"VLDataScienceMachineLearningPackage.qriteration","text":"qriteration(A::Array{Float64,2}; maxiter::Int64 = 10, tolerance::Float64 = 1e-9) -> Tuple\n\nComputes the eigenvalues and eigenvectors of a real matrix A using the QR iteration method.\n\nArguments\n\nA::Array{Float64,2}: a real matrix of size n x n.\nmaxiter::Int64: the maximum number of iterations (default is 10).\ntolerance::Float64: the tolerance for the stopping criterion (default is 1e-9).\n\nReturns\n\nTuple: a tuple of two elements: the first element is an array of eigenvalues and the second element is a dictionary of eigenvectors.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#VLDataScienceMachineLearningPackage.solve","page":"Linear","title":"VLDataScienceMachineLearningPackage.solve","text":"solve(A::AbstractMatrix{T}, b::AbstractVector{T}, xₒ::AbstractVector{T}; \nalgorithm::AbstractLinearSolverAlgorithm = JacobiMethod(), ϵ::Float64 = 0.01, maxiterations::Int64 = 100) where T <: Number\n\nThe solve function solves the linear system of equations Ax = b using the specified algorithm.  The function returns the solution vector x for each iteration of an iterative method. \n\nArguments\n\nA::AbstractMatrix{T}: The system matrix A in the linear system of equations Ax = b.\nb::AbstractVector{T}: The right-hand side vector b in the linear system of equations Ax = b.\nxₒ::AbstractVector{T}: The initial guess for the solution vector x.\nalgorithm::AbstractLinearSolverAlgorithm: The algorithm to use to solve the linear system of equations. The default algorithm is JacobiMethod().\nϵ::Float64: The error tolerance for the iterative method. The default value is 1e-6.\nmaxiterations::Int64: The maximum number of iterations for the iterative method. The default value is 1000.\nω::Float64: The relaxation factor for the Successive Over-Relaxation method. The default value is 1.0. This parameter is only used if the SuccessiveOverRelaxationMethod algorithm is selected.\n\nReturns\n\nd::Dict{Int,Array{T,1}}: The solution vector x for each iteration of an iterative method. The keys of the dictionary are the iteration numbers, and the values are the solution vectors at each iteration.\n\n\n\n\n\nsolve(problem::MyLinearProgrammingProblemModel) -> Dict{String,Any}\n\nSolves a linear programming problem defined by the MyLinearProgrammingProblemModel instance using the GLPK solver.\n\nArguments\n\nproblem::MyLinearProgrammingProblemModel: An instance of MyLinearProgrammingProblemModel holding the data for the problem.\nconstraints::Symbol: The type of constraints to apply. Options are :leq (less than or equal to), :geq (greater than or equal to), or :eq (equal to). Default is :leq.\n\nReturns\n\nDict{String,Any}: A dictionary with the following keys:\n\"argmax\": The optimal choice.\n\"budget\": The budget at the optimal choice.\n\"objective_value\": The value of the objective function at the optimal choice.\n\n\n\n\n\n","category":"function"},{"location":".ipynb_checkpoints/index-checkpoint/#VLDataScienceMachineLearningPackage.jl","page":"VLDataScienceMachineLearningPackage.jl","title":"VLDataScienceMachineLearningPackage.jl","text":"","category":"section"},{"location":".ipynb_checkpoints/index-checkpoint/","page":"VLDataScienceMachineLearningPackage.jl","title":"VLDataScienceMachineLearningPackage.jl","text":"The VLDataScienceMachineLearningPackage.jl package is a Julia package that provides functions and types useful for data science, machine learning, and artificial intelligence applications. The package is designed to be simple and easy to use, and it is suitable for students, researchers, and practitioners in the data science and machine learning areas.","category":"page"},{"location":".ipynb_checkpoints/index-checkpoint/#Installation","page":"VLDataScienceMachineLearningPackage.jl","title":"Installation","text":"","category":"section"},{"location":".ipynb_checkpoints/index-checkpoint/","page":"VLDataScienceMachineLearningPackage.jl","title":"VLDataScienceMachineLearningPackage.jl","text":"The package can be installed by running the following command in the Julia REPL:","category":"page"},{"location":".ipynb_checkpoints/index-checkpoint/","page":"VLDataScienceMachineLearningPackage.jl","title":"VLDataScienceMachineLearningPackage.jl","text":"using Pkg\nPkg.add(url=\"https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl.git\")","category":"page"},{"location":"binaryclassification/#Binary-Classification","page":"Binary Classification","title":"Binary Classification","text":"","category":"section"},{"location":"binaryclassification/","page":"Binary Classification","title":"Binary Classification","text":"We've implemented some basic functionality for binary classification tasks, including a simple perceptron model and logistic regression. This model can be trained on labeled data and used to classify new instances.","category":"page"},{"location":"binaryclassification/#VLDataScienceMachineLearningPackage.learn","page":"Binary Classification","title":"VLDataScienceMachineLearningPackage.learn","text":"learn(features::Array{<:Number,2}, labels::Array{<:Number,1}, algorithm::AbstractClassificationAlgorithm; \n    maxiter::Int64 = 100, verbose::Bool = false)\n\nThe function learns a classification model from the data provided using the algorithm specified. This is a wrapper function that calls the internal function _learn whose implementation is algorithm-specific.\n\nArguments\n\nfeatures::Array{<:Number,2}: the features.\nlabels::Array{<:Number,1}: the labels.\nalgorithm::AbstractClassificationAlgorithm: the algorithm to use to learn the model.\n\nReturns\n\nthe updated algorithm model.\n\n\n\n\n\n","category":"function"},{"location":"binaryclassification/#VLDataScienceMachineLearningPackage.classify","page":"Binary Classification","title":"VLDataScienceMachineLearningPackage.classify","text":"classify(features::Array{<:Number,2}, algorithm::AbstractClassificationAlgorithm)\n\n\n\n\n\nclassify(test::Array{<:Number,1}, algorithm::AbstractClassificationAlgorithm)\n\n\n\n\n\n","category":"function"},{"location":"binaryclassification/#VLDataScienceMachineLearningPackage.confusion","page":"Binary Classification","title":"VLDataScienceMachineLearningPackage.confusion","text":"confusion(actual::Array{Int64,1}, model::Array{Int64,1}) -> Array{Int64,2}\n\nThe function computes the confusion matrix for the classification model.\n\nArguments\n\nactual::Array{<:Number,1}: the actual labels.\nmodel::Array{<:Number,1}: the model estimated labels.\n\nReturns\n\na 2x2 confusion matrix. The rows correspond to the actual labels and the columns correspond to the predicted labels.\n\n\n\n\n\n","category":"function"},{"location":"data/#Data","page":"Data","title":"Data","text":"","category":"section"},{"location":"data/","page":"Data","title":"Data","text":"We've included several datasets in the package that we use for examples, activities, etc. ","category":"page"},{"location":"data/#VLDataScienceMachineLearningPackage.MyKaggleCustomerSpendingDataset","page":"Data","title":"VLDataScienceMachineLearningPackage.MyKaggleCustomerSpendingDataset","text":"MyKaggleCustomerSpendingDataset() -> DataFrame\n\nLoad the Kaggle customer spending dataset as a DataFrame.  The original dataset can be found at: Spending dataset.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MyStringDecodeChallengeDataset","page":"Data","title":"VLDataScienceMachineLearningPackage.MyStringDecodeChallengeDataset","text":"MyStringDecodeChallengeDataset() -> NamedTuple\n\nLoad the String Decode Challenge testing and production datasets. \n\nReturn\n\nNamedTuple: A tuple containing the three datasets:\ntest_part_1: The first part of the test dataset.\ntest_part_2: The second part of the test dataset.\nproduction: The production dataset.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MyCommonSurnameDataset","page":"Data","title":"VLDataScienceMachineLearningPackage.MyCommonSurnameDataset","text":"MyCommonSurnameDataset() -> DataFrame\n\nLoad the common surnames dataset by country as a DataFrame. The original dataset can be found at: Common Surnames by Country.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MyCommonForenameDataset","page":"Data","title":"VLDataScienceMachineLearningPackage.MyCommonForenameDataset","text":"MyCommonForenameDataset() -> DataFrame\n\nLoad the common forenames dataset by country as a DataFrame. The original dataset can be found at: Common Forenames by Country.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MyKaggleHousingPricesDataset","page":"Data","title":"VLDataScienceMachineLearningPackage.MyKaggleHousingPricesDataset","text":"MyHousingPricesDataset() -> DataFrame\n\nLoad the house prices dataset from Kaggle as a DataFrame. The original dataset can be found at: Housing Prices Dataset on Kaggle\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MySarcasmCorpus","page":"Data","title":"VLDataScienceMachineLearningPackage.MySarcasmCorpus","text":"function MySarcasmCorpus() -> MySarcasmRecordCorpusModel\n\nThe function corpus reads a file composed of JSON records and returns the data as a MySarcasmRecordCorpusModel instance. Each record in the file is expected to have the following fields:\n\nis_sarcastic::Bool - a boolean value indicating if the headline is sarcastic.\nheadline::String - the headline of the article.\narticle_link::String - the link to the article.\n\nReturns\n\nMySarcasmRecordCorpusModel - the data from the file as a MySarcasmRecordCorpusModel instance.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MySMSSpamHamCorpus","page":"Data","title":"VLDataScienceMachineLearningPackage.MySMSSpamHamCorpus","text":"function MySMSSpamHamCorpus() -> MySMSSpamHamRecordCorpusModel\n\nThe function MySMSSpamHamCorpus reads the SMS Spam Ham dataset and returns the data as a MySMSSpamHamRecordCorpusModel instance.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MyGraphEdgeModels","page":"Data","title":"VLDataScienceMachineLearningPackage.MyGraphEdgeModels","text":"function MyGraphEdgeModels(filepath::String, edgeparser::Function; comment::Char='#', \ndelim::Char=',')::Dict{Int64,MyGraphEdgeModel}\n\nFunction to parse an edge file and return a dictionary of edges models.\n\nArguments\n\nfilepath::String: The path to the edge file.\nedgeparser::Function: A callback function to parse each edge line. This function should take a line as input, and a delimiter character, and return a tuple of the form (source, target, data), where:\nsource::Int64: The source node ID.\ntarget::Int64: The target node ID.\ndata::Any: Any additional data associated with the edge, e.g., a weight, a tuple of information, etc.\n\nReturns\n\nDict{Int64,MyGraphEdgeModel}: A dictionary of edge models.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MyConstrainedGraphEdgeModels","page":"Data","title":"VLDataScienceMachineLearningPackage.MyConstrainedGraphEdgeModels","text":"function MyConstrainedGraphEdgeModels(filepath::String, edgeparser::Function; comment::Char='#', \n    delim::Char=',') -> Dict{Int64,MyConstrainedGraphEdgeModel}\n\nThis function parses a constrained graph edge file and returns a dictionary of constrained graph edge models.\n\nArguments\n\nfilepath::String: The path to the edge file.\nedgeparser::Function: A callback function to parse each edge line. This function should take a line as input, and a delimiter character, and return a tuple of the form (source, target, weight, lower, upper), where:\nsource::Int64: The source node ID.\ntarget::Int64: The target node ID.\nweight::Union{Nothing, Number}: The weight of the edge.\nlower::Union{Nothing, Number}: The lower bound of the edge weight.\nupper::Union{Nothing, Number}: The upper bound of the edge weight.\n\nReturns\n\nDict{Int64,MyConstrainedGraphEdgeModel}: A dictionary of constrained graph edge models.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.MyBanknoteAuthenticationDataset","page":"Data","title":"VLDataScienceMachineLearningPackage.MyBanknoteAuthenticationDataset","text":"MyBanknoteAuthenticationDataset() -> DataFrame\n\nThe second dataset we will explore is the banknote authentication dataset from the UCI archive.  This dataset has 1372 instances of 4 continuous features and an integer (-1,1) class variable. \n\n\n\n\n\n","category":"function"},{"location":"data/#Financial-Data","page":"Data","title":"Financial Data","text":"","category":"section"},{"location":"data/","page":"Data","title":"Data","text":"We have included a dataset of market data that we use for examples and activities in the financial domain. This  dataset holds the daily open, high, low, close, and volume data for a selection of stocks between 2014 and 2024.  In addition, we have defined a few methods for working with this data.","category":"page"},{"location":"data/#VLDataScienceMachineLearningPackage.MyTrainingMarketDataSet","page":"Data","title":"VLDataScienceMachineLearningPackage.MyTrainingMarketDataSet","text":"MyTrainingMarketDataSet() -> Dict{String, DataFrame}\n\nLoad the components of the SP500 Daily open, high, low, close (OHLC) dataset as a dictionary of DataFrames. This data was provided by Polygon.io and covers the period from January 3, 2014, to December 31, 2024.\n\n\n\n\n\n","category":"function"},{"location":"data/#VLDataScienceMachineLearningPackage.log_growth_matrix","page":"Data","title":"VLDataScienceMachineLearningPackage.log_growth_matrix","text":"function log_growth_matrix(dataset::Dict{String, DataFrame}, \n            firms::Array{String,1}; Δt::Float64 = (1.0/252.0), risk_free_rate::Float64 = 0.0) -> Array{Float64,2}\n\nThe log_growth_matrix function computes the excess log growth matrix for a given set of firms where we define the log growth as:\n\n    mu_tt-1(r_f) = frac1Delta t logleft(fracS_tS_t-1right) - r_f\n\nwhere S_t is the volume weighted average price (units: USD/share) at time t, Delta t is the time increment (in years), and r_f is the annual risk-free rate (units: 1/years) assuming continuous compounding.\n\nArguments\n\ndataset::Dict{String, DataFrame}: A dictionary of data frames where the keys are the firm ticker symbols and the values are the data frames holding price data. We use the volume_weighted_average_price column to compute the log growth by default.\nfirms::Array{String,1}: An array of firm ticker symbols for which we want to compute the log growth matrix.\nΔt::Float64: The time increment used to compute the log growth. The default value is 1/252, i.e., one trading day in units of years.\nrisk_free_rate::Float64: The risk-free rate used to compute the log growth. The default value is 0.0.\nkeycol::Symbol: The column in the data frame to use to compute the log growth. The default value is :volume_weighted_average_price.\ntestfirm::String: The firm ticker symbol to use to determine the number of trading days. By default, we use \"AAPL\".\n\nReturns\n\nArray{Float64,2}: An array of the excess log growth values for the given set of firms. The time series is the rows and the firms are the columns. The columns are ordered according to the order of the firms array.\n\nSee:\n\nThe DataFrame type (and methods for working with data frames) is exported from the DataFrames.jl package\n\n\n\n\n\n","category":"function"},{"location":"#VLDataScienceMachineLearningPackage.jl","page":"Home","title":"VLDataScienceMachineLearningPackage.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The VLDataScienceMachineLearningPackage.jl package is a Julia package that provides functions and types useful for data science, machine learning, and artificial intelligence applications. The package is designed to be simple and easy to use, and it is suitable for students, researchers, and practitioners in the data science and machine learning areas.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The package can be installed by running the following command in the Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(url=\"https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl.git\")","category":"page"},{"location":"types/#Types","page":"Types","title":"Types","text":"","category":"section"},{"location":"types/","page":"Types","title":"Types","text":"We'll work with may types in this package, including some abstract types that will be used to define the structure of our models.","category":"page"},{"location":"types/#VLDataScienceMachineLearningPackage.MySarcasmRecordModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MySarcasmRecordModel","text":"MySarcasmRecordModel <: AbstractTextRecordModel\n\nModel for a record in the Sarcasm dataset.\n\nFields\n\ndata::Array{String, Any} - The data found in the record in the order they were found\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MySarcasmRecordCorpusModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MySarcasmRecordCorpusModel","text":"MySarcasmRecordCorpusModel <: AbstractTextDocumentCorpusModel\n\nModel for a collection of records in the Sarcasm dataset.\n\nFields\n\nrecords::Dict{Int, MySarcasmRecordModel} - The records in the document (collection of records)\ntokens::Dict{String, Int64} - A dictionary of tokens in alphabetical order (key: token, value: position) for the entire document\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MySMSSpamHamRecordModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MySMSSpamHamRecordModel","text":"MySMSSpamHamRecordModel <: AbstractTextRecordModel\n\nModel for a record in the SMS Spam Ham dataset.\n\nFields\n\nisspam::Bool - a boolean value indicating if the message is spam.\nmessage::String - the content of the SMS message.\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MySMSSpamHamRecordCorpusModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MySMSSpamHamRecordCorpusModel","text":"MySMSSpamHamRecordCorpusModel <: AbstractTextDocumentCorpusModel\n\nModel for a collection of records in the SMS Spam Ham dataset.\n\nFields\n\nrecords::Dict{Int, MySMSSpamHamRecordModel} - The records in the document (collection of records)\ntokens::Dict{String, Int64} - A dictionary of tokens in alphabetical order (key: token, value: position) for the entire document\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MyFullGeneralAdjacencyTree","page":"Types","title":"VLDataScienceMachineLearningPackage.MyFullGeneralAdjacencyTree","text":"mutable struct MyFullGeneralAdjacencyTree <: AbstractTreeModel\n\nThe MyFullGeneralAdjacencyTree type is a model of a full general adjacency tree that uses a dictionary to store the tree structure. There is a build and populate! method to build the tree and populate it with data.\n\nFields\n\ndata::Union{Nothing, Dict{Int64,NamedTuple}} - A dictionary that stores the node data for the tree.\nconnectivity::Dict{Int64,Array{Int64,1}} - A dictionary that stores the connectivity information between nodes.\nh::Int64 - The height of the tree.\nn::Int64 - The branching factor of the tree.\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MyAdjacencyRecombiningCommodityPriceTree","page":"Types","title":"VLDataScienceMachineLearningPackage.MyAdjacencyRecombiningCommodityPriceTree","text":"mutable struct  MyAdjacencyRecombiningCommodityPriceTree <: AbstractPriceTreeModel\n\nThe MyAdjacencyRecombiningCommodityPriceTree type is a model of a commodity price tree that uses an dictionary to store the price data. This model stores the connectivity information between nodes.\n\nFields\n\ndata::Union{Nothing, Dict{Int64,NamedTuple}} - A dictionary that stores the price data and path informationfor the tree.\nconnectivity::Dict{Int64,Array{Int64,1}} - A dictionary that stores the connectivity information between nodes.\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MyOneDimensionalElementaryWolframRuleModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MyOneDimensionalElementaryWolframRuleModel","text":"mutable struct MyOneDimensionalElementaryWolframRuleModel <: AbstractRuleModel\n\nThe MyOneDimensionalElementarWolframRuleModel mutable struct represents a one-dimensional elementary Wolfram rule model.\n\nFields\n\nindex::Int - The index of the rule\nradius::Int - The radius, i.e, the number of cells that influence the next state for this rule\nrule::Dict{Int,Int} - A dictionary that holds the rule where the key is the binary representation of the neighborhood and the value is the next state\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MyGraphNodeModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MyGraphNodeModel","text":"mutable struct MyGraphNodeModel\n\nA lightweight mutable node model used in simple graph representations.\n\nFields\n\nid::Int64 - Unique integer identifier for the node.\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MyGraphEdgeModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MyGraphEdgeModel","text":"mutable struct MyGraphEdgeModel\n\nA mutable edge model representing a directed or undirected connection between nodes. The model stores a numeric id, endpoint indices, and an optional numeric weight.\n\nFields\n\nid::Int64 - Unique integer identifier for the edge.\nsource::Int64 - Identifier of the source node (or one endpoint).\ntarget::Int64 - Identifier of the target node (or the other endpoint).\nweight::Union{Nothing, Any} - Optional edge weight; nothing indicates an unweighted edge.\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MySimpleDirectedGraphModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MySimpleDirectedGraphModel","text":"mutable struct MySimpleDirectedGraphModel\n\nA minimal mutable directed graph container that keeps node and edge registries and a children adjacency map for fast traversal of outgoing neighbors.\n\nFields\n\nnodes::Union{Nothing, Dict{Int64, MyGraphNodeModel}} - Optional mapping from node id to MyGraphNodeModel. Use nothing when uninitialized.\nedges::Union{Nothing, Dict{Tuple{Int, Int}, Int64}} - Optional mapping from (source, target) tuple to edge id. Use nothing when uninitialized.\nchildren::Union{Nothing, Dict{Int64, Set{Int64}}} - Optional adjacency map from a node id to the set of its child (outgoing) node ids.\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MySimpleUndirectedGraphModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MySimpleUndirectedGraphModel","text":"mutable struct MySimpleUndirectedGraphModel\n\nA minimal mutable undirected graph container that keeps node and edge registries and a children adjacency map for fast traversal of outgoing neighbors.\n\nFields\n\nnodes::Union{Nothing, Dict{Int64, MyGraphNodeModel}} - Optional mapping from node id to MyGraphNodeModel. Use nothing when uninitialized.\nedges::Union{Nothing, Dict{Tuple{Int, Int}, Int64}} - Optional mapping from (source, target) tuple to edge id. Use nothing when uninitialized.\nchildren::Union{Nothing, Dict{Int64, Set{Int64}}} - Optional adjacency map from a node id to the set of its child (outgoing) node ids.\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MyDirectedBipartiteGraphModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MyDirectedBipartiteGraphModel","text":"mutable struct MyDirectedBipartiteGraphModel <: AbstractGraphModel\n\nThis type models a directed bipartite graph with source and sink nodes, along with maximum capacity constraints on the edges. This type is constructed using a build method.\n\nFields\n\nnodes::Union{Nothing, Dict{Int64, MyGraphNodeModel}} - Optional mapping from node id to MyGraphNodeModel. Use nothing when uninitialized.\nedges::Union{Nothing, Dict{Tuple{Int, Int}, Int64}} - Optional mapping from (source, target) tuple to edge id. Use nothing when uninitialized.\nchildren::Union{Nothing, Dict{Int64, Set{Int64}}} - Optional adjacency map from a node id to the set of its child (outgoing) node ids.\nedgesinverse::Dict{Int, Tuple{Int, Int}} - Map between edge id and (source, target) tuple.\nleft::Set{Int64} - Set of left (source) node ids.\nright::Set{Int64} - Set of right (sink) node ids.\nsource::Int64 - Source node id.\nsink::Int64 - Sink node id.\ncapacity::Dict{Tuple{Int64, Int64}, Tuple{Number, Number}} - Capacity constraints on the edges.\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MyConstrainedGraphEdgeModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MyConstrainedGraphEdgeModel","text":"mutable struct MyConstrainedGraphEdgeModel\n\nMutable model for a graph edge with capacity constraints. \n\nFields\n\nid::Int64 - Unique identifier for the edge.\nsource::Int64 - Identifier for the source node.\ntarget::Int64 - Identifier for the target node.\nlower::Union{Nothing, Number} - Lower capacity constraint for the edge.\nupper::Union{Nothing, Number} - Upper capacity constraint for the edge.\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MyPerceptronClassificationModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MyPerceptronClassificationModel","text":"mutable struct MyPerceptronClassificationModel <: AbstractClassificationAlgorithm\n\nA mutable struct that represents a perceptron classification model.\n\nFields\n\n- `β::Vector{Float64}`: coefficients\n- `mistakes::Int64`: number of mistakes that are are willing to make\n\n\n\n\n\n","category":"type"},{"location":"types/#VLDataScienceMachineLearningPackage.MyLinearProgrammingProblemModel","page":"Types","title":"VLDataScienceMachineLearningPackage.MyLinearProgrammingProblemModel","text":"mutable struct MyLinearProgrammingProblemModel <: AbstractLinearProgrammingProblemType\n\nA mutable struct that represents a linear programming problem model.\n\nFields\n\nA::Array{Float64,2}: constraint matrix\nb::Array{Float64,1}: right-hand side vector\nc::Union{Array{Float64,2}, Array{Float64,1}}: objective function coefficient matrix (vector)\nlb::Array{Float64,1}: lower bound vector\nub::Array{Float64,1}: upper bound vector  \n\n\n\n\n\n","category":"type"}]
}
